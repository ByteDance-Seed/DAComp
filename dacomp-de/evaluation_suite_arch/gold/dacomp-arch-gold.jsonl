{"id": "dacomp-de-arch-001", "question": "Why does our cost per hire for some positions reach as high as $50,000, while for others it's only $5,000? I've heard this is related to our 'candidate quality prediction' and 'recruiting channel ROI optimization', but right now, we're operating completely as a black box. I need a system that can calculate the true cost of recruitment in real-time, considering not only hidden costs like time costs, interviewer salaries, and the impact on candidate experience, but also one that can predict which candidates are worth spending a significant amount to pursue and which channels should have their budgets cut. The board of directors wants to see specific data and optimization recommendations next month.", "rubric": "# Recruiting Cost & Candidate Value Optimization Evaluation Criteria (Strict & Complex Version)\n\n## [Total Score | 34 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (27 Points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n## Requirement I: Business Alignment & Semantic Accuracy (27 Points)\n\n### Criterion 1.1 (Max 24 Points): Feature Point Coverage\n- Path 1.1.A (24 Points): **Complete Coverage**\n  - Sub-criterion 1.1.A.1: 【1 Point】Candidate Experience Transparency Metric｜Judgment: `transparency_score` exists (numeric 0–100; not_null; tests exist).\n  - Sub-criterion 1.1.A.2: 【1 Point】Interview Experience Quality｜Judgment: `interview_experience_score` exists (numeric 0–100; type DOUBLE; tests exist).\n  - Sub-criterion 1.1.A.3: 【1 Point】Interview Burden Category｜Judgment: `interview_burden_category` exists (enum {Low,Medium,High}; tests constrain enum).\n  - Sub-criterion 1.1.A.4: 【1 Point】Interview Consistency Score｜Judgment: `interview_consistency_score` exists (numeric 0–1; consistent across tables).\n  - Sub-criterion 1.1.A.5: 【1 Point】Outcome Fairness Score｜Judgment: `outcome_fairness_score` exists (range 0–100).\n  - Sub-criterion 1.1.A.6: 【1 Point】Timeline Experience Score｜Judgment: `timeline_experience_score` exists (range 0–100).\n  - Sub-criterion 1.1.A.7: 【1 Point】Overall Candidate Experience Score｜Judgment: `overall_candidate_experience_score` exists (must be an aggregate column, equal to a weighted or combined value of the preceding experience metrics).\n  - Sub-criterion 1.1.A.8: 【1 Point】Experience Grade｜Judgment: `experience_grade` exists (enum A–E or High/Medium/Low).\n  - Sub-criterion 1.1.A.9: 【1 Point】Interview Completion Rate %｜Judgment: `interview_completion_rate_pct` exists (range 0–100; tests `BETWEEN`).\n  - Sub-criterion 1.1.A.10: 【1 Point】Average Interview Duration｜Judgment: `avg_interview_duration_minutes` exists (≥0; unit: minutes; tests `non_negative`).\n  - Sub-criterion 1.1.A.11: 【1 Point】Days from Application to Last Contact/Advancement｜Judgment: `days_from_application_to_last_contact`, `days_since_last_advancement` exist (≥0; unit: days).\n  - Sub-criterion 1.1.A.12: 【1 Point】Total Interviews｜Judgment: `total_interviews` exists (≥0 integer).\n  - Sub-criterion 1.1.A.13: 【1 Point】Candidate Quality Distribution & Quantiles｜Judgment: `overall_avg_quality_score`, `median_quality_score`, `p75_quality_score` exist; and the numeric range is fixed at 0–5 or 0–100 (tests must be consistent).\n  - Sub-criterion 1.1.A.14: 【1 Point】Best Channel/Department/Level｜Judgment: `best_source_type`, `best_department`, `best_job_level` exist; value domain has enum/hierarchy definition.\n  - Sub-criterion 1.1.A.15: 【1 Point】Funnel Conversion Rate Chain｜Judgment: `application_to_screening_rate`→`offer_to_hired_rate` exists; intermediate stages are complete and in the correct order.\n  - Sub-criterion 1.1.A.16: 【1 Point】Average Funnel Duration｜Judgment: `avg_days_to_screening`, `avg_days_screening_to_technical`, `avg_total_days_to_hire` exist; all are ≥0.\n  - Sub-criterion 1.1.A.17: 【1 Point】Funnel Bottleneck Identifier/Efficiency Grade｜Judgment: `primary_bottleneck` (enum of stage names), `hiring_efficiency_grade` (grade enum) exist.\n  - Sub-criterion 1.1.A.18: 【1 Point】Posting Performance & Engagement Metrics｜Judgment: `posting_analytics` contains at least `total_applications`, `unique_candidates`, `total_interview_time_minutes`; 0 points if any one is missing.\n  - Sub-criterion 1.1.A.19: 【1 Point】Posting Conversion Rate Family｜Judgment: `application_to_interview_rate`, `application_to_offer_rate`, `application_to_hire_rate`, `offer_to_hire_rate` exist; definitions/scopes are consistent.\n  - Sub-criterion 1.1.A.20: 【1 Point】Days to First Application for Posting｜Judgment: `days_to_first_application` exists (≥0; unit: days).\n  - Sub-criterion 1.1.A.21: 【1 Point】Department Hierarchy Dimension Set｜Judgment: `department`/`team`/`location`/`level`/`commitment_type` exist, and all are non-null and joinable.\n  - Sub-criterion 1.1.A.22: 【1 Point】Department Cost-Efficiency Tier｜Judgment: `cost_efficiency_tier` exists (enum High/Medium/Low); tests enum.\n  - Sub-criterion 1.1.A.23: 【1 Point】Manager Performance Dashboard Key Ratios｜Judgment: `avg_posting_conversion_rate`, `candidate_hire_rate`, `overall_performance_score`, `performance_tier` exist; tests range is non-negative.\n  - Sub-criterion 1.1.A.24: 【1 Point】Stage History & Time Boundaries｜Judgment: `valid_from`, `valid_ending_at` exist; must ensure time does not overlap and the range is reasonable.\n\n- Path 1.1.B (3 Points): **Basic Coverage**\n  - Sub-criterion 1.1.B.1: 【1 Point】Entity Coverage｜Judgment: At least three of `Opportunity`/`Posting`/`Interview`/`Department`/`HiringManager` have primary keys and core columns.\n  - Sub-criterion 1.1.B.2: 【1 Point】Time Dimension Coverage｜Judgment: At least two of `report_date`/`generated_at`/`valid_from`/`valid_ending_at` exist.\n  - Sub-criterion 1.1.B.3: 【1 Point】Aggregation/Statistics Coverage｜Judgment: At least 3 percentage or average metrics (e.g., conversion_rate, avg_days, rate_pct); otherwise 0 points.\n\n### Criterion 1.2 (Max 3 Points): Metric/Feature Scope\n- Path 1.2.A (3 Points): **Clear Scope**\n  - Sub-criterion 1.2.A.1: 【1 Point】Output items must include \"subject + time\"｜Example: `lever__hiring_dashboard`'s `report_date` + count of the last 30 days.\n  - Sub-criterion 1.2.A.2: 【1 Point】Exception/Missing Value Handling｜Judgment: tests constrain at least two places (e.g., not_null, unique, non-negative, time cannot be in the future).\n  - Sub-criterion 1.2.A.3: 【1 Point】Consistent scope across output items｜Judgment: Funnel conversion rates in `int_lever__hiring_funnel_metrics` and `posting_analytics` are both percentages (same denominator/chain); otherwise 0 points.\n\n## Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n\n### Criterion 2.1 (Max 2 Points): Format/Structure Check\n- Path 2.1.A (2 Points): **Essential Model Elements**\n  - Sub-criterion 2.1.A.1: 【1 Point】Model identifier/dependencies/granularity｜Judgment: The three elements `name`, `source_models`, `grain` must exist.\n  - Sub-criterion 2.1.A.2: 【1 Point】Field definition + type｜Judgment: All columns under `columns` must contain `name`+`data_type`; 0 points if missing.\n\n### Criterion 2.2 (Max 2 Points): Dependency Closure\n- Path 2.2.A (2 Points): **Buildable Dependency Graph**\n  - Sub-criterion 2.2.A.1: 【1 Point】No cycles/self-references｜Judgment: mart only depends on intermediate/staging; 0 points if a loop is found.\n  - Sub-criterion 2.2.A.2: 【1 Point】Clear boundaries for external sources｜Judgment: Source entities (opportunity/posting/interview) in the stg layer must be explicitly referenced in the `source_models` of the intermediate layer; otherwise 0 points.\n\n## Requirement III: Design Quality (3 Points)\n\n### Criterion 3.1 (Max 1 Point): Clear Naming\n- Sub-criterion 3.1.1: 【1 Point】Naming Convention｜Judgment: Uniformly use snake_case; layering prefixes `stg_`/`int_`/`lever__`; double underscore separates system and subject; otherwise 0 points.\n\n### Criterion 3.2 (Max 2 Points): Reasonable Layering\n- Sub-criterion 3.2.1: 【1 Point】Clear layer responsibilities｜Judgment: intermediate aggregates business scopes, marts only output analytical scopes; otherwise 0 points.\n- Sub-criterion 3.2.2: 【1 Point】Avoid \"God tables\"｜Judgment: Job postings, candidates, managers, departments are split into separate models; a single table does not mix multiple subjects; otherwise 0 points.\n\n## Scoring Notes\n- Each sub-criterion is strictly scored as **1/0**.\n- **No evidence (field/SQL/contract/tests) means 0 points**.\n- ALLOW_MULTI_MODEL=YES: Division of labor across multiple models is allowed, but evidence for each scored item must be locatable within a single model; inference across models will not be scored."}
{"id": "dacomp-de-arch-002", "question": "We are facing a very real problem. The board of directors is reviewing our product investment strategy next month, and they want to know which of our 100+ applications are profitable and which are not. More importantly, we need to redefine our application portfolio strategy based on user behavior data and revenue contribution. Right now, Sales says some enterprise customers are threatening to downgrade to the Pro plan, Product says user engagement data looks good, but Finance says there's an issue with the ROI calculation. I need an analysis that integrates these three dimensions—user behavior, application cost, and customer revenue potential—to tell us the true business value of each application. It should also tell us which applications we should increase investment in, and which we should consider discontinuing or merging. This analysis must be able to support our case to the board, demonstrating that our product decisions are data-driven and not based on gut feelings.", "rubric": "# Real Business Value Assessment of Application Portfolio Evaluation Criteria (Strict & Complex Version)\n\n## [Total Score | 35 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (28 Points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n## Requirement I: Business Alignment & Semantic Accuracy (28 Points)\n\n### Standard 1.1 (Max 20 Points): Feature Point Coverage\n- Path 1.1.A (20 Points): **Complete Coverage**\n  - Sub-standards (1 point per item, awarded if met; evidence must be explicitly present in `docs/gold.yaml` or model definitions; scored as 0 if it cannot be located):\n    1. Subsidiary/Customer entity identifier【1 Point｜Criteria】Presence of `account_id` and `visitor_id`, and can establish foreign key or join mappings with `app_id/feature_id/page_id`.\n    2. Fiscal period (daily) alignment【1 Point｜Criteria】Presence of `date_day` or `occurred_on`, which can serve as a daily-granularity primary key; example reference: `int_pendo__calendar_spine.date_day`.\n    3. User activity and stickiness metrics【1 Point｜Criteria】Fields must include `count_active_visitors`, `count_active_days`, `sessions_per_user`, `avg_session_duration`, and all must be ≥0.\n    4. Funnel conversion metrics【1 Point｜Criteria】Presence of `page_to_feature_conversion_rate`, `feature_to_deep_conversion_rate`, `overall_conversion_rate`, with a value range of 0–1 or 0–100.\n    5. Retention metrics【1 Point｜Criteria】Presence of `retention_7d`, `retention_30d`, `retention_90d`; the three must appear as a group.\n    6. NPS and satisfaction metrics【1 Point｜Criteria】Presence of `net_promoter_score` and at least two related sub-items (`promoter_rate`, `detractor_rate`, `total_nps_responses`, `latest_nps_rating`).\n    7. Feature adoption and depth-of-use metrics【1 Point｜Criteria】Presence of `feature_adoption_rate`, `feature_depth_adoption_rate`, supplemented by `avg_features_per_user` or `sum_clicks`.\n    8. Page usage and stickiness metrics【1 Point｜Criteria】Presence of `sum_pageviews`, `avg_visitor_pageviews`, `percent_of_daily_pageviews`.\n    9. Account-level daily aggregation【1 Point｜Criteria】Presence of `pendo__account_daily_metrics` or `int_pendo__account_daily_metrics` (grain: account_id + date_day).\n    10. User lifecycle segmentation and churn risk【1 Point｜Criteria】Presence of `lifecycle_stage`, `engagement_trend`, `churn_risk_level`.\n    11. RFM and value tiering【1 Point｜Criteria】Presence of `recency_score`, `frequency_score`, `magnitude_score`, and has `rfm_segment` or `customer_value_tier`.\n    12. Customer importance/influence【1 Point｜Criteria】Presence of `customer_importance_level` or `account_influence_level`; the enumerated field must have a definition.\n    13. High-value/high-risk account aggregation【1 Point｜Criteria】Presence of aggregated outputs such as `high_risk_accounts`, `medium_risk_accounts`, `low_risk_accounts`, or `excellent_revenue_accounts`.\n    14. Investment suggestions/corrective actions【1 Point｜Criteria】Presence of `optimization_recommendation`, `recommended_strategy`, `recommended_next_action`, or `investment_expectation`.\n    15. App/feature/page inter-mapping【1 Point｜Criteria】Presence of `app_id`, `feature_id`, `page_id`, and they are joinable at the marts layer.\n    16. Contribution percentage/share【1 Point｜Criteria】Presence of `percent_of_daily_feature_clicks`, `percent_of_daily_visitors`, `percent_of_daily_accounts`; the range must be 0–1 or a percentage.\n    17. Behavioral intensity and deep engagement metrics【1 Point｜Criteria】Presence of `power"}
{"id": "dacomp-de-arch-003", "question": "We've noticed that in recent months, a significant number of high-value customers stop all system activity within 30-90 days after converting to a paid plan, appearing as if they have churned. Strangely, these customers show high engagement in `Marketo`, `Stripe` indicates they are still paying normally, yet there are almost no support request records for them in `Zendesk`. I need you to build a complete \"Silent Customer Risk Warning System\" capable of identifying these customers who appear normal but may have underlying issues, and of predicting their potential churn risk by analyzing changes in behavioral patterns across systems.", "rubric": "# Silent Customer Risk Warning System (Customer360) Evaluation Criteria (Strictly Numbered Version)\n\n## [Total Score | 34 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (27 Points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n> General Scoring Principles\n> - Evidence First: Points are awarded only for **locatable evidence** in fields/YAML/SQL/tests/data dictionary; any \"speculation/verbal explanation/screenshot without field names\" receives 0 points.\n> - Type and Scope: If the required **type/scope/enum** is missing or inconsistent with the description, 0 points are given.\n> - Time Granularity: Timestamps must be of type `timestamp`; daily granularity must be of type `date`; the meaning of time windows must be documented.\n> - **Mutual Exclusion Note**: 1.1.A and 1.1.B are **mutually exclusive** (score one or the other). The maximum score for 1.1 is 24 points.\n\n\n## Requirement I: Business Alignment & Semantic Accuracy (27 Points)\n\n### Standard 1.1 (Max 24 Points): Feature Point Coverage\n- Path 1.1.A (24 Points): **Complete Coverage** (1 point per item)\n  - 1.1.A.1 [1 Point] Identity resolution three-source primary keys: `marketo_lead_id`, `stripe_customer_id`, and `zendesk_user_id` exist (all non-null and linkable); any missing → 0 points.\n  - 1.1.A.2 [1 Point] Three-source time fields: `created_at` and `updated_at` (timestamp) exist for each source, with timezone specified; any missing or type mismatch → 0 points.\n  - 1.1.A.3 [1 Point] Three-channel engagement: `marketo_engagement`, `stripe_engagement`, and `zendesk_engagement` exist (numeric ≥0; definition is locatable).\n  - 1.1.A.4 [1 Point] Composite engagement score: `composite_engagement_score` exists (calculation logic/weights are locatable; range constraints in tests).\n  - 1.1.A.5 [1 Point] Channel coverage flags: `in_marketo`, `in_stripe`, and `in_zendesk` exist (boolean {true,false}; enum/type is consistent).\n  - 1.1.A.6 [1 Point] Organization and contact information quality: `organization_data_quality`, `has_contact_phone`, and `address_data_quality` exist (value domain is documented).\n  - 1.1.A.7 [1 Point] Time spent across stages: `marketing_to_sales_days` and `sales_to_support_days` exist (unit=days, ≥0; calculation path is traceable).\n  - 1.1.A.8 [1 Point] Account age/maturity: `account_age_days` (≥0) and `customer_maturity` (enum with layered definitions) exist.\n  - 1.1.A.9 [1 Point] Journey pattern: `customer_journey_pattern` exists (enum or clustering label; generation logic is locatable).\n  - 1.1.A.10 [1 Point] Payment reliability: `payment_reliability` exists (score/level, value domain documented).\n  - 1.1.A.11 [1 Point] Segmentation and risk level: `customer_segment` and `churn_risk_level` exist (enum set is fixed; consistency tests).\n  - 1.1.A.12 [1 Point] Recommended action: `recommended_action` exists (enum/copy template, source logic is locatable).\n  - 1.1.A.13 [1 Point] Lifecycle metrics - window: `customer_last_activity` (timestamp) and `active_30/60/90/180/365_days` (boolean/numeric, window definition is documented) exist.\n  - 1.1.A.14 [1 Point] Days since last activity: `days_since_last_activity` exists (≥0; consistency tests with `last_activity`).\n  - 1.1.A.15 [1 Point] Platform consistency/health score: `platform_consistency_score` and `customer_health_score` exist (scope and algorithm description are locatable).\n  - 1.1.A.16 [1 Point] Value and retention: `estimated_customer_value`, `estimated_customer_ltv` (numeric ≥0), `retention_probability`, `churn_probability` (0–1), and `revenue_at_risk` (≥0) exist.\n  - 1.1.A.17 [1 Point] Risk composition and classification: `risk_category`, `primary_risk_factor`, `risk_trend`, and `total_risk_score` exist, covering at least two of `activity_decline/platform_abandonment/lifecycle_transition/email_engagement`.\n  - 1.1.A.18 [1 Point] Payment status and stability: `is_delinquent`, `stripe_deleted` (boolean), `payment_behavior_risk`, and `account_stability_risk` exist (level/score definition is locatable).\n  - 1.1.A.19 [1 Point] Support status and time: `zendesk_active`, `zendesk_suspended`, `zendesk_deleted` (boolean), `organization_id` (linkable) exist, and include status timestamps.\n  - 1.1.A.20 [1 Point] Intervention recommendation and success rate: `recommended_intervention`, `intervention_success_probability` (0–1), `intervention_timeline`, and `intervention_roi` (≥0; algorithm description is locatable) exist.\n  - 1.1.A.21 [1 Point] High-value risk / critical alerts: `critical_alert_flag`, `high_value_at_risk_flag`, and `engagement_warning_flag` exist (boolean; rule documentation is locatable).\n  - 1.1.A.22 [1 Point] Unified primary key and mapping: `customer360_id`, `customer360_organization_id`, and `source_ids` (contains the three source IDs and their respective timestamps; foreign key integrity tests) exist.\n  - 1.1.A.23 [1 Point] Customer master table information: Includes email, name, organization, address, phone, and summaries for marketing/sales/support (fields are complete, types are standardized).\n  - 1.1.A.24 [1 Point] Detailed domain table coverage: Domain tables for address/email/name/organization/phone/status/summary/update tracking exist and have a primary key and update timestamp (SCD/change strategy is described).\n\n- Path 1.1.B (3 Points): **Basic Coverage (Mutually exclusive with 1.1.A, only used if A is not met)**\n  - 1.1.B.1 [1 Point] Entity coverage: Provides at least a unified ID mapping and any one source system ID (e.g., `customer360__mapping` is available and joinable).\n  - 1.1.B.2 [1 Point] Time coverage: Provides at least one of: last activity, windowed activity, or time spent across stages (e.g., `days_since_last_activity` or `active_30/60/90` or `marketing_to_sales_days`).\n  - 1.1.B.3 [1 Point] Aggregation/scoring type: Provides at least one aggregation/score from `engagement/health/risk/LTV` (e.g., `composite_engagement_score`).\n\n> Note: The maximum score for 1.1 is 24 points. If the score for 1.1.A is > 0, then the score for 1.1.B is not counted.\n\n### Standard 1.2 (Max 3 Points): Metric/Feature Definitions\n- Path 1.2.A (3 Points): **Clear Definitions** (1 point per item)\n  - 1.2.A.1 [1 Point] Granularity + Time/Definition: Each output item **includes at least two types** of elements (subject ID / time range or alignment / statistical or aggregation rule). Example: Identity resolution includes the subject and `created_at`/`updated_at`; behavior/lifecycle includes windows or stages. Missing one type → 0 points.\n  - 1.2.A.2 [1 Point] Handling of anomalies/missing/duplicates/negative values/future times: **Default handling and ignore strategies are explicitly defined** in documentation or tests (e.g., deletion/retention flags, email quality filtering, time cannot be in the future, numeric values are non-negative, primary key deduplication rules). Any missing → 0 points.\n  - 1.2.A.3 [1 Point] Cross-item consistency: The same concepts (e.g., \"last activity/account age/engagement score\") have **consistent** naming, units, denominators/numerators, and time granularities across different outputs/models; inconsistent → 0 points.\n\n## Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n\n### Standard 2.1 (Max 2 Points): Format/Structure Check\n- Path 2.1.A (2 Points): **Minimum Four Elements of a Model**\n  - 2.1.A.1 [1 Point] Field definitions: All models provide `columns` (field name + data type + comment/meaning); missing columns or types → 0 points.\n  - 2.1.A.2 [1 Point] Test constraints: Tests exist (uniqueness/not-null/referential integrity/range/enum); any core key without tests → 0 points.\n\n### Standard 2.2 (Max 2 Points): Dependency Closure\n- Path 2.2.A (2 Points): **Buildable Dependency Graph**\n  - 2.2.A.1 [1 Point] No cycles/self-references: The dependency graph can be topologically sorted (raw/stg → int → marts); cycle detected → 0 points.\n  - 2.2.A.2 [1 Point] External source boundaries: raw/stg/int/marts layers are clear, and sources are locatable in documentation/directory structure; layer skipping or cross-layer writing is prohibited; non-compliant → 0 points.\n\n## Requirement III: Design Quality (3 Points)\n\n### Standard 3.1 (Max 1 Point): Clear Naming\n- 3.1.1 [1 Point] Consistent naming conventions: Suffixes like `*_updated_at`, `*_created_at`, `*_engagement` are consistent; `snake_case` is used; system/topic prefixes are consistent; inconsistent → 0 points.\n\n### Standard 3.2 (Max 2 Points): Reasonable Layering\n- 3.2.1 [1 Point] Clear layer responsibilities: Boundaries for raw (source)/stg (staging)/int (intermediate)/marts (analytics exposure) are clear (verifiable by path or prefix).\n- 3.2.2 [1 Point] Avoid \"God tables\": Address/email/phone/status/summary are split into domain tables; modules are cohesive, dependencies are unidirectional and clear; mixed or cross-domain stacking → 0 points.\n\n## Scoring Instructions\n- **1/0 Scoring**: Each sub-item must have locatable evidence (fields, SQL, YAML, tests, data dictionary).\n- **Strict Constraints**: Inconsistencies in type/scope/enum/time granularity/timezone/granularity → immediate 0 points.\n- **ALLOW_MULTI_MODEL=YES**: Collaborative coverage by multiple models is allowed, but **must** find explicit evidence for each scored item in at least one model/document. \"Reasonable inference\" across models will not be scored."}
{"id": "dacomp-de-arch-004", "question": "Can we create a 'true performance profile' for each sales representative? I want to know not just how much money they've sold, but more importantly, what is the quality of the customers they've acquired. Will these customers continue to do business with us in the future? Do specific actions during the sales process (such as the pace of advancing opportunities, customer communication frequency, etc.) affect a customer's long-term value?", "rubric": "# Sales Rep \"True Performance Profile\" Evaluation Criteria\n\n## [Total Score | 19 Points] Three Fixed Requirements\n- Requirement I: Business Alignment and Semantic Accuracy (12 points)\n- Requirement II: Technical Feasibility and Structural Completeness (4 points)\n- Requirement III: Design Quality (3 points)\n\n## Requirement I: Business Alignment and Semantic Accuracy (12 points)\n\n### Criterion 1.1 (Max 9 points): Functional Point Coverage\n- Path 1.1.A (9 points): Complete Coverage\n  - Sub-criterion (Subsidiary | 1 point): Has account hierarchy or subsidiary identifiers, enabling analysis by subsidiary dimension. Condition: The `grain` corresponds to account granularity, and a hierarchy column equivalent to `parent_id` is included.\n  - Sub-criterion (Fiscal Period | 1 point): Has fiscal period fields (`fiscal_quarter`, `fiscal_year`) for period-based analysis. Condition: The model includes fiscal quarter and fiscal year columns or synonymous fields.\n  - Sub-criterion (Cash Flow | 1 point): Has tables or columns for cash flow (inflow/outflow) analysis. Condition: Cash flow-related metrics/columns are present → 0 points if missing.\n  - Sub-criterion (AR Aging | 1 point): Has accounts receivable balance and aging bucket fields. Condition: AR aging-related columns are present → 0 points if missing.\n  - Sub-criterion (Gross Profit | 1 point): Has revenue and cost (or cost of goods sold) to support gross profit/margin analysis. Condition: Revenue and cost-related columns are present → 0 points if missing.\n  - Sub-criterion (Expenses | 1 point): Has various expense fields (e.g., sales/administrative/operating) to support expense analysis. Condition: Expense-related columns are present → 0 points if missing.\n  - Sub-criterion (Supplier Payments | 1 point): Has supplier payment records or accounts payable-related fields. Condition: Supplier payment-related columns are present → 0 points if missing.\n  - Sub-criterion (Contribution Percentage | 1 point): Has contribution or percentage fields (e.g., by sales rep/customer/product dimension). Condition: Contribution percentage-related columns are present → 0 points if missing.\n  - Sub-criterion (Correction Actions | 1 point): Has data quality correction action configurations (`correct`/`delete_row`/`nullify_field`) with rules defined at the field level. Condition: Quality action and field-level rule keys are present.\n\n- Path 1.1.B (3 points): Basic Coverage (Entity/Time/Aggregation)\n  - Sub-criterion (Entity Coverage | 1 point): Includes column definitions for at least two entity models: accounts and contacts. Condition: Models and columns for account and contact entities are present.\n  - Sub-criterion (Time Coverage | 1 point): Has a time dimension aligned to a date/snapshot (e.g., `snapshot_date` or `created_date`). Condition: Date/snapshot dimension columns are present.\n  - Sub-criterion (Aggregation Coverage | 1 point): Has columns or derivable columns corresponding to output metrics like count/sum/average (e.g., conversion rate, quantity, amount).\n\n### Criterion 1.2 (Max 3 points): Metric/Feature Definitions\n- Path 1.2.A (3 points): Clear Definitions\n  - Sub-criterion (Completeness of Definition Elements | 1 point): Each output item includes at least two of the following elements: \"by whom\" (e.g., account/contact/opportunity/sales rep), time range or alignment (e.g., `snapshot_date`/`fiscal_year`), and statistical rule or metric type. Condition: Model/column descriptions explicitly state the subject and time elements.\n  - Sub-criterion (Handling of Anomalies/Missing Values | 1 point): Provides default handling or constraints for missing/anomalous/duplicate/future time values (refer to field-level `validation_rules` in `docs/data_contract.yaml`, e.g., delete rows with future dates/handle nulls). Condition: Corresponding rule keys exist and cover key fields like date/boolean/text.\n  - Sub-criterion (Cross-Model Definition Consistency | 1 point): Identical or synonymous fields have consistent definitions across different models. Condition: No conflicting descriptions for the same concept.\n\n## Requirement II: Technical Feasibility and Structural Completeness (4 points)\n\n### Criterion 2.1 (Max 2 points): Format/Structure Check\n- Path 2.1.A (2 points): Minimum Four Model Elements\n  - Sub-criterion (Upstream Dependencies `source_models` | 1 point): Declares upstream models (e.g., pointing to staging or intermediate). Condition: Each business model provides an upstream reference key.\n  - Sub-criterion (Granularity Definition `grain` | 1 point): A `grain` key exists and its meaning is clear (e.g., \"one row per account per day,\" \"one row per contact per day,\" \"one row per lead\"). Condition: All models have a `grain`.\n\n### Criterion 2.2 (Max 2 points): Dependency Closure\n- Path 2.2.A (2 points): Buildable Dependency Graph\n  - Sub-criterion (No Circular/Self-References | 1 point): The dependency graph is acyclic. Condition: References between models do not form cycles or self-references.\n  - Sub-criterion (External Source Boundaries | 1 point): References to external sources (e.g., `raw.*`) require clear boundary descriptions and scope. Condition: All external sources are noted.\n\n## Requirement III: Design Quality (3 points)\n\n### Criterion 3.1 (Max 1 point): Clear Naming\n- Sub-criterion (Semantic Naming | 1 point): Model and column names are semantically clear. Condition: No jargon/ambiguous naming.\n\n### Criterion 3.2 (Max 2 points): Logical Layering\n- Sub-criterion (Clear Layer Responsibilities | 1 point): Clear responsibility boundaries for raw/staging/intermediate/marts layers. Condition: Layer responsibilities are well-described.\n- Sub-criterion (Centralized Business Definitions | 1 point): Business rules are centralized in the appropriate layer. Condition: Not scattered.\n\n### Scoring Notes\n- Each sub-criterion is scored as 1 or 0; no evidence → 0 points.\n- `ALLOW_MULTI_MODEL=YES`."}
{"id": "dacomp-de-arch-005", "question": "We have a major problem with our ad accounts right now: we are completely unable to connect the behavior of the same user across different devices. For example, if a user sees our ad on their phone, then searches and converts on their computer, we conclude that the mobile ad was ineffective and the desktop ad was highly successful. Based on this calculation, we might be incorrectly discontinuing many effective ad placements and possibly over-investing in channels with 'inflated' performance.\n\nNow, the CFO is demanding more accurate ROI data, and I need a system for cross-device user journey analysis. I want to know: What devices does a user cross between from the first ad impression to the final conversion? Which ads truly serve the 'seeding' (awareness-building) role? Which ones play the 'harvesting' (conversion) role? How should we allocate our budget among these touchpoints with different functions?\n\nThe key is to be able to identify the cross-device behavior path of a single user and then re-evaluate the true value of each touchpoint. Our current reports show a very low ROI for mobile, but it's possible that mobile is serving an `awareness` function, while the actual `conversion` happens on `desktop`.", "rubric": "# dacomp-de-arch-005-google_ads-001-1 Evaluation Criteria\n\n## [Total Score | 38 Points] Three Fixed Requirements\n- Requirement I: Business Alignment and Semantic Accuracy (31 points)\n- Requirement II: Technical Feasibility and Structural Completeness (4 points)\n- Requirement III: Design Quality (3 points)\n\n## Requirement I: Business Alignment and Semantic Accuracy (31 points)\n\n### Criterion 1.1 (Max 28 points): Functional Point Coverage\n- Path 1.1.A (28 points): Complete Coverage\n  - [1 point | Passing Condition] An account-level daily summary model exists with a granularity of \"per account per day\".\n  - [1 point | Passing Condition] The account report includes `date_day` and it is not null.\n  - [1 point | Passing Condition] Account dimension fields are complete: `account_id`, `account_name`.\n  - [1 point | Passing Condition] Account currency/timezone fields: `currency_code` (restricted enum), `time_zone`.\n  - [1 point | Passing Condition] Account-level core metrics: `spend`, `clicks`, `impressions`.\n  - [1 point | Passing Condition] Account-level conversion metrics: `conversions`, `conversions_value`, `view_through_conversions`.\n  - [1 point | Passing Condition] A campaign-level daily model exists with a granularity of \"per campaign per day\".\n  - [1 point | Passing Condition] Campaign dimension fields: `campaign_id`, `campaign_name`, `status`.\n  - [1 point | Passing Condition] Campaign channel fields: `advertising_channel_type`/`subtype`.\n  - [1 point | Passing Condition] Campaign-level metrics: `spend`, `clicks`, `impressions`, `conversions`, `conversions_value`, `view_through_conversions`.\n  - [1 point | Passing Condition] An ad group-level daily model exists, including `ad_group_id`/`name`/`type`/`status`.\n  - [1 point | Passing Condition] Ad group-level metrics are complete: `spend`, `clicks`, `impressions`, `conversions`, `conversions_value`, `view_through_conversions`.\n  - [1 point | Passing Condition] An ad-level daily model exists, including `ad_id`, `ad_name`, `ad_type`, `status`, `display_url`.\n  - [1 point | Passing Condition] Ad-level metrics are complete: `spend`, `clicks`, `impressions`, `conversions`, `conversions_value`, `view_through_conversions`.\n  - [1 point | Passing Condition] A keyword-level daily model exists, including `criterion_id` (`keyword_id`), `keyword_match_type` (restricted enum), `keyword_text`.\n  - [1 point | Passing Condition] Keyword-level metrics are complete: `spend`, `clicks`, `impressions`, `conversions`, `conversions_value`, `view_through_conversions`.\n  - [1 point | Passing Condition] A search term-level daily model exists, including `search_term`, `search_term_match_type` (restricted enum) and is associated with `criterion_id`.\n  - [1 point | Passing Condition] Search term-level metrics are complete: `spend`, `clicks`, `impressions`, `conversions`, `conversions_value`, `view_through_conversions`.\n  - [1 point | Passing Condition] A URL-level daily model exists, including `base_url`, `url_host`, `url_path`, and the five UTM parameters.\n  - [1 point | Passing Condition] URL-level metrics are complete: `spend`, `clicks`, `impressions`, `conversions`, `conversions_value`, `view_through_conversions`.\n  - [1 point | Passing Condition] A channel-device analysis model exists, including the three dimensions: `advertising_channel_type`/`subtype`, `ad_network_type`, `device`.\n  - [1 point | Passing Condition] Channel-device level core metrics are complete: `total_spend`/`clicks`/`impressions`/`conversions`/`conversions_value`.\n  - [1 point | Passing Condition] Contribution share (Share) metrics are complete: `spend_share_pct`, `clicks_share_pct`, `conversions_share_pct`.\n  - [1 point | Passing Condition] Channel/device benchmarks and comparisons: `channel_benchmark_ctr`/`roas`, `device_benchmark_ctr`/`cpa`, and `vs_%` metrics.\n  - [1 point | Passing Condition] Channel mix health and tiering: `active_campaigns`, `lifecycle` category counts, `top`/`bottom`/`underperforming` and their corresponding percentages.\n  - [1 point | Passing Condition] Investment tier and strategy output: `channel_investment_tier`, `channel_efficiency_score`, `strategic_recommendation`, various alert flags.\n  - [1 point | Passing Condition] Customer acquisition analysis: `acquisition_cohort`, `days_since_first_conversion`, `account`/`customer_maturity_stage`.\n  - [1 point | Passing Condition] Acquisition efficiency and payback: `customer_acquisition_cost`, `average_customer_ltv`, `ltv_cac_ratio`, `estimated_payback_days`.\n  - [1 point | Passing Condition] Cumulative acquisition and benchmarking: `cumulative_acquisition_cost`/`conversions`/`ltv`, `cohort_avg_cac`/`ltv`, `vs_%`, percentile and tiering.\n  - [1 point | Passing Condition] Acquisition strategy and alerts: `strategic_customer_segment`, `acquisition_recommendation`, `negative_roi_alert`, `high_cac_alert`, `scale_opportunity`, `retention_risk`.\n  - [1 point | Passing Condition] Executive dashboard time alignment: includes week-over-week and month-over-month changes.\n  - [1 point | Passing Condition] Executive dashboard structural health: `premium` investment and percentage, campaign/keyword counts, average efficiency score, overall performance score/tier, and multiple types of alerts.\n\n- Path 1.1.B (3 points): Basic Coverage (If 1.1.A is not met, score based on basic coverage)\n  - [1 point | Passing Condition] At least two or more entity dimensions are covered (e.g., any two of account/campaign/keyword/channel/device).\n  - [1 point | Passing Condition] The time dimension has daily granularity and includes at least one type of period-over-period comparison metric (either WoW or MoM).\n  - [1 point | Passing Condition] The aggregation scope covers at least one core metric group (any one group of impressions/clicks/spend/conversions, containing ≥2 metrics).\n\n### Criterion 1.2 (Max 3 points): Metric/Feature Definitions\n- Path 1.2.A (3 points): Clear Definitions\n  - [1 point | Passing Condition] Each output item contains at least two types of elements: \"by whom (entity dimension) + time range (daily granularity)\" or \"alignment rule + statistical definition\".\n  - [1 point | Passing Condition] Provides default handling or constraints for missing/abnormal/negative values (e.g., `not_null`, `>=0`, enum tests exist; example: `currency_code` enum).\n  - [1 point | Passing Condition] Definitions are consistent across output items (e.g., enums like `device`/`advertising_channel_type`/`status` maintain consistent constraints across all models).\n\n## Requirement II: Technical Feasibility and Structural Completeness (4 points)\n\n### Criterion 2.1 (Max 2 points): Format/Structure Check\n- Path 2.1.A (2 points): Minimum Four Elements of a Model\n  - [1 point | Passing Condition] Each model has a model identifier `name`, and each model has an explicit granularity `grain`.\n  - [1 point | Passing Condition] Each model provides field definitions + types (`columns` contain `name` and `data_type`, throughout all models).\n\n### Criterion 2.2 (Max 2 points): Dependency Closure\n- Path 2.2.A (2 points): Buildable Dependency Graph\n  - [1 point | Passing Condition] No circular/self-references (the model DAG has no cycles).\n  - [1 point | Passing Condition] Clear boundaries for external sources (`stg_*` is considered output from an external cleaning or upstream stage; when referenced, it must have a clear source name and not use mixed aliases).\n\n## Requirement III: Design Quality (3 points)\n\n### Criterion 3.1 (Max 1 point): Clear Naming\n- [1 point | Passing Condition] Consistent abbreviations/naming conventions (e.g., using a consistent style like `advertising_channel_type`, `view_through_conversions`).\n\n### Criterion 3.2 (Max 2 points): Reasonable Layering\n- [1 point | Passing Condition] Centralized business definitions (e.g., definitions like `cohort`, `maturity_stage`, `efficiency`/`score` are centralized in corresponding thematic models, not scattered).\n- [1 point | Passing Condition] Avoid \"God tables\"; modules are cohesive, dependencies are clear (thematic models are aggregated around entities/problems, do not pile unrelated definitions into a single table).\n\n### Scoring Notes\n- Each sub-criterion is scored on a 1/0 basis; no evidence found → 0 points.\n- `ALLOW_MULTI_MODEL=YES`."}
{"id": "dacomp-de-arch-006", "question": "Our CFO urgently approached me yesterday, stating that an audit revealed serious issues with our advertising budget execution. The board-approved annual advertising budget of $12 million has already seen $9.8 million spent by August, yet we've only achieved 65% of our revenue target. What's worse, we discovered that the same customer converting on different platforms was counted multiple times, leading to severely distorted ROAS data. The CFO has demanded the immediate establishment of a \"Budget Burn Rate Alert System\" capable of real-time monitoring of budget efficiency for each platform, forecasting end-of-month and end-of-year budget overrun risks, and identifying which marketing campaigns are \"burning money without generating profit.\" She said that if she doesn't see a concrete budget control plan by next Monday, she will consider suspending some advertising placements.", "rubric": "# Ad Budget Burn Rate Alert System Evaluation Criteria\n\n## [Total Score | 26 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (19 Points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n> **Scoring Principles**\n> - **Evidence First**: Points are awarded only if the field exists and can be located in the model definition (YAML/SQL/data dictionary/tests); verbal inferences do not count.\n> - **Strong Constraints**: Numeric fields must declare non-negativity/range, enum fields must list valid values, and time fields must have a type definition.\n> - **Clear Grain**: All outputs involving time/entities must declare their grain (e.g., \"per account_id per day\").\n> - **No evidence found → 0 points**.\n\n\n## Requirement I: Business Alignment & Semantic Accuracy (19 Points)\n\n### Criterion 1.1 (Up to 16 Points): Feature Point Coverage\n- Path 1.1.A (16 Points): Complete Coverage (1 point each)\n\n  - [1 Point | Platform Dimension] Presence of `platform` field, a non-null string enum with a value range like `{\"google_ads\", \"facebook_ads\", ...}`; enum validation required in tests.\n  - [1 Point | Account Dimension] Presence of `account_id` field, type string/integer, non-null, unique, and joinable with the"}
{"id": "dacomp-de-arch-007", "question": "We have recently discovered huge disparities in user retention rates across different regions and device types, and the patterns of these disparities are completely inconsistent between the two app store platforms. For instance, our game app performs very well on iOS devices in Japan, but users on Android devices hardly ever return. What's worse, we are unable to accurately track the true value contribution of users during their cross-platform usage. Investors are now requiring us to provide a comprehensive analysis report at the next quarterly board meeting on the correlation between user retention and value creation. This report must explain why we are investing heavily in certain market segments for minimal returns, and how to optimize our user acquisition portfolio. This analysis needs to consider the complete user lifecycle path, regional cultural differences, and device preferences, and must also be able to predict the future value potential of different user cohorts.", "rubric": "# App Reporting (User Retention & Value Analysis) Evaluation Criteria (Strict Version)\n\n## [Total Score | 42 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (35 Points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n> General Scoring Rules\n> - **Evidence First**: Points are awarded only when a specific field/constraint can be located in the model's YAML/SQL/data dictionary/tests. Inferences or informal descriptions do not count.\n> - **Strong Type/Range Constraints**: Metric fields must conform to numeric/percentage/boolean/enum requirements, otherwise they are scored 0 points.\n> - **Mutual Exclusion Note**: Score either 1.1.A (Complete Coverage) or 1.1.B (Basic Coverage); they cannot be combined.\n> - **Cross-Table Consistency**: The same field across different models must be consistent in definition/unit/range/enum, otherwise it is scored 0 points.\n\n## Requirement I: Business Alignment & Semantic Accuracy (35 Points)\n\n### Criterion 1.1 (Max 32 Points): Feature Point Coverage\n- Path 1.1.A (32 Points): **Complete Coverage** (1 point per item)\n  - 1.1.A.1 App Entity and Platform Dimension【1 Point】Presence of `app_id`/`app_name`/package name fields + `platform`, with platform enum"}
{"id": "dacomp-de-arch-008", "question": "We invest significant labor costs in project management each year, but which of these investments truly generate value, and which are pure waste? She found that we lack a complete \"project investment return analysis system.\" For example, we need to know the true cost structure of each project from initiation to delivery (labor input, time cost, collaboration cost, etc.), which project types have the highest investment efficiency, the cost-benefit differences between projects of varying teams and complexities, and most critically—how to build a \"project investment decision model\" based on historical data to predict its return on investment and resource requirements before a project even starts. She has requested that before the next quarterly budget meeting, we establish a complete project financial analysis system that can support future project investment decisions and resource allocation optimization.", "rubric": "# Project Investment & Resource Decision Analysis (Asana) Evaluation Criteria (Strict Version)\n\n## [Total Score | 38 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (31 Points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n> **Scoring Principles**\n> - **Evidence-First**: Points are awarded only if the field can be located in the model definition (YAML/SQL/schema/tests).\n> - **Strong Constraints**: Numeric fields must declare non-negativity/range, enum fields must list valid values, and time fields must have type and granularity definitions.\n> - **Explicit Grain**: The grain, such as \"per project per day\" or similar, must be declared in the model documentation.\n> - **No evidence → 0 points**.\n\n## Requirement I: Business Alignment & Semantic Accuracy (31 Points)\n\n### Standard 1.1 (Max 28 Points): Feature Point Coverage\n- Path 1.1.A (28 Points): Complete Coverage (1 point per item)\n  - [1 Point] Project-level investment efficiency: Field `roi_efficiency_ratio`, DOUBLE ≥0, with tests to verify non-negativity.\n  - [1 Point] Project task scale: Field `total_tasks`, INT ≥0.\n  - [1 Point] Completion rate: Field `completion_percentage`, 0–100%.\n  - [1 Point] Number of overdue tasks: Field `overdue_tasks`, INT ≥0.\n  - [1 Point] Number of active tasks: Field `active_tasks`, INT ≥0.\n  - [1 Point] Number of unique assignees: Field `unique_assignees`, INT ≥0.\n  - [1 Point] Collaboration score: Field `collaboration_score`, 0–100.\n  - [1 Point] Efficiency score: Field `efficiency_score`, 0–100.\n  - [1 Point] Risk percentage: Field `risk_percentage`, 0–100%.\n  - [1 Point] Average tasks per assignee: Field `avg_tasks_per_assignee`, DOUBLE ≥0.\n  - [1 Point] Progress/efficiency/health rank: Fields `progress_rank`, `efficiency_rank`, `health_rank`, INT ≥1, with the metric explicitly defined as a rank.\n  - [1 Point] Complexity factor: Field `complexity_factor` (DOUBLE ≥0) or `project_complexity_level` (enum Low/Medium/High).\n  - [1 Point] Schedule forecast: Field `schedule_forecast` or `project_forecast`, type JSON/STRING, must include a description of the forecast time range.\n  - [1 Point] Management priority: Field `management_priority`, enum High/Medium/Low.\n  - [1 Point] Improvement suggestions and focus areas: Fields `improvement_opportunities`, `focus_areas`, `success_factors`, STRING/ARRAY.\n  - [1 Point] Remaining/estimated duration: Field `estimated_days_to_completion` or `remaining_days`, INT ≥0.\n  - [1 Point] Project deadline dimension: Field `project_deadline`, DATE, with tests to check if it is after the current time.\n  - [1 Point] Team size category: Field `team_size_category`, enum {Small, Medium, Large}.\n  - [1 Point] Project maturity phase: Field `project_maturity_phase`, enum {Initiation, Growth, Mature, Decline}.\n  - [1 Point] Investment analysis - total hours invested: Field `total_investment_hours`, DOUBLE ≥0.\n  - [1 Point] Investment analysis - ROI: Field `actual_roi_ratio`, DOUBLE ≥0.\n  - [1 Point] Investment analysis - output efficiency: Field `output_per_hour`, DOUBLE ≥0.\n  - [1 Point] Investment analysis - investment per person: Field `investment_per_person`, DOUBLE ≥0.\n  - [1 Point] Cost structure percentage: Fields `direct_cost_percentage`, `collaboration_cost_percentage`, `management_cost_percentage`, each 0–100%, with a sum ≤100.\n  - [1 Point] Risk-adjusted grade and budget recommendation: Field `risk_adjusted_investment_grade` (enum A/B/C) or `budget_recommendation` (STRING).\n  - [1 Point] Investment priority and key insights: Field `investment_priority_score` (DOUBLE ≥0) or `key_insight` (STRING).\n  - [1 Point] Future investment advice: Field `future_investment_advice`, STRING.\n  - [1 Point] Predictive factors - success prediction and tiering: Field `roi_success_prediction` (probability 0–1), `expected_roi_tier` (enum High/Medium/Low).\n  - [1 Point] Predictive factors - primary risk factor and recommendation: Field `primary_risk_factor` (STRING), `investment_recommendation` (STRING).\n  - [1 Point] Predictive factors - time-series labels: Field `start_quarter` (Q1–Q4), `season` (enum Spring/Summer/Fall/Winter), `timing_type` (STRING).\n  - [1 Point] Decision model - success rate and expected return: Field `success_rate` (0–1), `return_expectation` (DOUBLE ≥0).\n  - [1 Point] Decision model - investment volatility and average investment: Field `roi_volatility` (DOUBLE ≥0), `avg_investment_hours` (DOUBLE ≥0).\n  - [1 Point] Decision model - resource allocation and risk assessment: Field `allocation_recommendation` (STRING), `risk_assessment` (STRING/enum).\n  - [1 Point] New project prediction template - investment and success probability: Fields `estimated_investment_hours` (≥0), `expected_roi` (≥0), `success_probability` (0–1).\n  - [1 Point] New project prediction template - team efficiency and per-person target: Fields `recommended_team_efficiency` (0–100), `target_investment_per_person` (≥0).\n  - [1 Point] New project prediction template - confidence level: Field `prediction_confidence`, 0–1.\n  - [1 Point] User monthly productivity trend: Fields `activity_month` (YYYY-MM), `completion_rate_pct` (0–100), `avg_completion_time` (≥0), `avg_health_score` (0–100).\n  - [1 Point] Daily task activity: Fields `date_day` (DATE), `tasks_created` (≥0), `tasks_completed` (≥0), `net_task_change` (can be positive or negative).\n\n- Path 1.1.B (3 Points): Basic Coverage (choose any, non-cumulative)\n  - [1 Point] Entity coverage: Has key fields for at least two entity types (project/task/team/user).\n  - [1 Point] Time coverage: Has at least `date_day` or `activity_month`.\n  - [1 Point] Aggregation coverage: Has at least 1 ratio-type field (e.g., `completion_percentage`) + 1 total or average-type field (e.g., `total_investment_hours`).\n\n### Standard 1.2 (Max 3 Points): Metric/Feature Definition\n- Path 1.2.A (3 Points)\n  - [1 Point] Each output item must include \"by whom + time + aggregation metric\"; the grain must be declared in the documentation.\n  - [1 Point] Anomaly and missing value handling: Requires an explicit column (e.g., `is_valid_record`/`anomaly_flag`) or tests to check for negative values, future dates, and duplicate records.\n  - [1 Point] Cross-table consistency: For example, the definitions for `completion_percentage` and `completion_rate_pct` must be consistent (denominator = total number of tasks).\n\n\n## Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n\n### Standard 2.1 (Max 2 Points): Format/Structure Check\n- [1 Point] Each model has a unique `name` and `source_models`.\n- [1 Point] Each model has `columns`, and all fields have a `data_type`.\n\n### Standard 2.2 (Max 2 Points): Dependency Closure\n- [1 Point] No circular dependencies or self-references.\n- [1 Point] External sources have clear boundary definitions (`raw_`/`stg_`/`int_`/`marts_`).\n\n\n## Requirement III: Design Quality (3 Points)\n\n### Standard 3.1 (Max 1 Point): Naming Clarity\n- [1 Point] Consistent naming: `avg_` prefix for averages, `pct` for percentages, `rank` for rankings, and no mixed spellings for ROI fields.\n\n### Standard 3.2 (Max 2 Points): Logical Layering\n- [1 Point] Centralized business logic: Investment/ROI logic is concentrated in `int_asana__project_investment_metrics`, `asana__project_investment_analysis`, and `asana__investment_decision_model`.\n- [1 Point] Avoid god tables: Entity/subject tables are clearly separated, and aggregation logic exists only in the mart layer.\n\n## Scoring Notes\n- Each sub-standard is scored strictly as 1/0; no evidence or no tests → 0 points.\n- **ALLOW_MULTI_MODEL=YES**: Points can be awarded if multiple models collectively meet a criterion, but clear evidence must be found in at least one model."}
{"id": "dacomp-de-arch-009", "question": "Our marketing manager, Ms. Li, has recently been struggling with a problem. The company has three applications: a gaming app, a social app, and an education app, with a fixed monthly promotion budget of $100,000. Now, the boss requires us to optimize the budget allocation, but we've encountered a very practical problem: although the gaming app has the highest number of downloads, the education app has more paying users; while the social app has good user activity, it constantly suffers from refunds. What's more complicated is that we've found the customer acquisition costs vary greatly across different channels - users from Google Play search are high-quality but few in number, while users from recommendation algorithms are numerous but have low retention rates. Now, Ms. Li has to make budget allocation decisions every month but lacks data support. She needs to know:\n\n1) What is the average revenue generated per user acquired from different channels for each app?\n2) Which channel has the highest return on investment (ROI)?\n3) If more budget is allocated to a specific app or channel, what is the expected increase in revenue?\n4) Is the current budget allocation reasonable, and are there any areas of obvious waste?\n\nShe hopes to have a simple and clear monthly report to help her make data-driven budget allocation decisions.", "rubric": "# Google Play Marketing Budget Allocation Analysis Evaluation Criteria (Strict Version)\n\n## [Total Score | 31 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (24 points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 points)\n- Requirement III: Design Quality (3 points)\n\n> General Scoring Rules (Applicable to the entire table)\n> - **Evidence First**: Points are awarded only when the field and its definition can be located in the model's YAML/SQL/data dictionary or tests; verbal explanations/inferences do not count.\n> - **Strong Type/Range Constraints**: Percentage/ratio ranges, codes/enums, and date/time types must meet requirements; non-compliance results in 0 points.\n> - **Clear Grain**: For any metric or derived dimension mentioned as \"daily/weekly/monthly,\" a clear grain description and calculation logic must be present in the documentation.\n\n## Requirement I: Business Alignment & Semantic Accuracy (24 points)\n\n### Standard 1.1 (Max 22 points): Feature Point Coverage\n- Path 1.1.A (22 points): Complete Coverage (1 point per item; each must satisfy \"field exists + correct type/range + locatable evidence,\" otherwise 0 points)\n\n  - [1 point | Condition] Application identifier `package_name` exists as one of the unified primary key dimensions; `package_name` is a non-null string, joinable across models, and has uniqueness/non-null tests.\n  - [1 point | Condition] Daily granularity `date_day` exists, with type DATE, serving as a key time dimension; the grain for the relevant fact table is documented as \"per package_name per day\".\n  - [1 point | Condition] Weekly/monthly time dimensions for \"monthly reports\": at least one of `day_name`, `week_of_year`, or `month_name` exists; type/value range is correct (`week_of_year` is between 1–53, with range tests).\n  - [1 point | Condition] Channel dimension `traffic_source` exists; enums or dictionary values (e.g., organic/paid/referral) are listed in the documentation; null value handling strategy is clearly defined.\n  - [1 point | Condition] Store listing visitors `store_listing_visitors` exists, integer ≥0; aggregation logic and deduplication rules are documented.\n  - [1 point | Condition] Store listing installers `store_listing_installers` exists, integer ≥0; definition is consistent with visitors (same granularity).\n  - [1 point | Condition] Store listing conversion rate `store_listing_conversion_rate` exists, range **0–1** (or 0–100%, one or the other, must be globally consistent); tests must validate the range.\n  - [1 point | Condition] Daily net revenue `daily_net_revenue` exists, numeric type ≥0; definition of net amount is locatable (e.g., `gross - refunds`).\n  - [1 point | Condition] Daily gross/total revenue `daily_gross_revenue` exists, numeric type ≥0; currency unit is fixed in the documentation (e.g., USD).\n  - [1 point | Condition] Daily refunds `daily_refunds` exists, numeric type ≥0; negative numbers are prohibited (tests).\n  - [1 point | Condition] Refund rate `refund_rate` exists, range **0–1** (or 0–100%, must be consistent); denominator definition (order count/revenue amount) is specified in the documentation.\n  - [1 point | Condition] Daily paying transactions `daily_paying_transactions` exists, integer ≥0; can be recalculated in relation to revenue-related fields (allowing ≤0.1% error).\n  - [1 point | Condition] Average transaction value `average_transaction_value` exists, numeric type ≥0; calculation formula (net amount/transaction count or gross amount/transaction count) is documented.\n  - [1 point | Condition] Moving average of net revenue exists: at least one of `avg_7d_net_revenue` or `avg_30d_net_revenue`; rolling window definition and boundary handling (for incomplete window periods) are documented.\n  - [1 point | Condition] Retention rate (at least `day_30_retention_rate`) exists; range is 0–1 or 0–100%, globally consistent with other rates/ratios.\n  - [1 point | Condition] Daily churn rate `daily_churn_rate` exists, range 0–1 (or 0–100%); calculation logic (definition of denominator and time window) is locatable.\n  - [1 point | Condition] Quality/stability and score: `daily_crashes` or `daily_anrs` (integer ≥0) and `quality_score` (0–100) exist; scoring algorithm/weighting explanation is locatable.\n  - [1 point | Condition] Country dimension `country` exists, ISO 2-letter code (length=2 tests); case specification is consistent (e.g., uppercase).\n  - [1 point | Condition] Revenue-side `buyer_country` exists, used for alignment with market country definitions; an alignment strategy is documented (e.g., rules for summarizing into `country` or displaying separately).\n  - [1 point | Condition] Week-over-week growth rate exists: at least one of `wow_install_growth_rate` or `wow_revenue_growth_rate`; calculation logic (last week vs. this week, week definition/timezone) is locatable; range allows negative values, unit (ratio or percentage) is consistent.\n  - [1 point | Condition] Business health/overall score exists: `business_health_status` (enum) or `overall_performance_score` (0–100); tiering/mapping rules are locatable.\n  - [1 point | Condition] Investment/campaign recommendation tag exists: at least one of `investment_recommendation` or `investment_priority`; enum or text template and trigger logic explanation are locatable.\n\n### Standard 1.2 (Max 2 points): Metric/Feature Definitions\n- Path 1.2.A (2 points): Clear Definitions (1 point per item; missing any requirement results in 0 points)\n  - [1 point | Condition] **Object + Time/Rules**: Each output item includes at least two of the following elements, which are locatable in the documentation:  \n    - Object (e.g., `package_name`/`traffic_source`/`country`);  \n    - Time (`date_day`/weekly/monthly alignment and `DATE_TRUNC` rules);  \n    - Statistical/aggregation rules (numerator/denominator/deduplication/rolling window).  \n    The grain must be explicitly stated in YAML/comments/SQL (e.g., \"per package_name per day\").\n  - [1 point | Condition] **Exception/Missing/Boundary Handling**: Tests or documentation clearly define at least three of the following constraints and default handling procedures:  \n    - Ratio/percentage ranges (e.g., `refund_rate` ∈ [0,1] or [0,100] with a globally unique definition);  \n    - Score/index ranges (e.g., `quality_score`, `overall_performance_score` ∈ [0,100]);  \n    - Code length and format (`country` length=2, ISO standard);  \n    - Time enums (`week_of_year` ∈ [1,53]);  \n    - Monetary amounts and counts are non-negative;  \n    - Handling strategy for missing values/duplicate records (deduplication primary key and sorting rules).\n\n\n## Requirement II: Technical Feasibility & Structural Completeness (4 points)\n\n### Standard 2.1 (Max 2 points): Format/Structure Check\n- Path 2.1.A (2 points): Minimum Four Model Elements (1 point per item)\n  - [1 point | Condition] Each model declares `name`, `grain`, and `source_models`; the grain must be clear (e.g., per package_name per day / per package_name per country per day).\n  - [1 point | Condition] The field list and `data_type` are defined in `columns` (at least one model must provide a complete column list example); missing field types/descriptions result in 0 points.\n\n### Standard 2.2 (Max 2 points): Dependency Closure\n- Path 2.2.A (2 points): Buildable Dependency Graph (1 point per item)\n  - [1 point | Condition] **No Circular/Self-References**: `int_*` models only serve as upstream sources for `marts_*` models; the dependency graph can be topologically sorted; discovery of loops/self-dependencies → 0 points.\n  - [1 point | Condition] **Clear External Source Boundaries**: Only known prefixes are referenced (e.g., `stg_google_play__*` / `int_google_play__*`), and no undefined external dependencies exist; source boundaries can be located in the directory or documentation.\n\n## Requirement III: Design Quality (3 points)\n\n### Standard 3.1 (Max 1 point): Clear Naming\n- [1 point | Condition] The same concept does not use mixed definitions: the units/ranges of fields like `refund_rate`, `average_rating`, and `quality_score` are consistent across models; if a `rate` field is sometimes 0–1 and other times 0–100% without a unified declaration → 0 points.\n\n### Standard 3.2 (Max 2 points): Reasonable Layering\n- [1 point | Condition] Clear layer responsibilities: `staging → intermediate → marts`; staging is responsible for cleaning and standardization, intermediate for aggregating single-subject definitions, and marts for scenario-oriented output; this can be proven by documentation/directory structure.\n- [1 point | Condition] Modular cohesion and clear dependencies: `int_*` models aggregate a **single subject** (behavior/revenue/retention/quality/store), while `marts_*` models combine multiple subjects for output; if an `int_*` model mixes multiple subjects or has bidirectional dependencies → 0 points.\n\n## Scoring Notes\n- **Scoring is 1/0 per item**; **no evidence found → 0 points**.  \n- **ALLOW_MULTI_MODEL=YES**: Multiple models can collectively satisfy the requirements, but each item must be **precisely locatable** (field and definition) in at least one model/YAML/SQL/test."}
{"id": "dacomp-de-arch-010", "question": "There is a severe problem with our company's email marketing return on investment (ROI). Although we send hundreds of thousands of marketing emails each month, we've found that many high-value potential customers have never received our emails, while some repeat customers are still receiving promotional emails intended for new clients. To make matters worse, we've discovered significant audience overlap between different marketing campaigns. The same person might receive 5-6 emails with similar content within a single week, leading to a high number of unsubscribes. We now need to establish a precise marketing audience management and email conflict avoidance system that can dynamically adjust email content and sending frequency based on the customer's lifecycle stage, purchase history, and behavioral characteristics, ensuring that each customer receives the most relevant email at the right time while avoiding excessive communication and content repetition.", "rubric": "# Precision Marketing Audience Management & Email Conflict Avoidance Evaluation Criteria\n\n## [Total Score | 25 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (18 Points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n## Requirement I: Business Alignment & Semantic Accuracy (18 Points)\n\n### Criterion 1.1 (Max 12 Points): Functional Point Coverage\n- Path 1.1.A (12 Points): Complete Coverage\n  - Sub-criterion: [1 Point | Condition] Presence of a unique contact identifier and deduplication key (e.g., `contact_id`, `calculated_merged_vids`, `email`), with granularity of \"one row per contact\" (`hubspot__contacts` and `int_hubspot__contact_merge_adjust`).\n  - Sub-criterion: [1 Point | Condition] Includes contact lifecycle segments and conversion status (e.g., `contact_maturity_segment`, `conversion_status`), and can be linked to creation/first conversion dates (`created_date`, `first_conversion_date`), with granularity of \"one row per contact\" (`hubspot__contact_engagement_analysis` and `int_hubspot__contact_lifecycle_metrics`).\n  - Sub-criterion: [1 Point | Condition] Has aggregations of email behavior events (`opens`, `clicks`, `bounces`, `spam_reports`, `deliveries`), which can be summarized by contact (`int_hubspot__email_event_aggregates`, `hubspot__contact_engagement_analysis`).\n  - Sub-criterion: [1 Point | Condition] Has records for individual email send events and status booleans (`was_delivered`, `was_opened`, `was_clicked`, `was_bounced`, `was_spam_reported`, `was_unsubscribed`), including `email_send_timestamp` (`hubspot__email_sends` / `hubspot__email_event_sent`).\n  - Sub-criterion: [1 Point | Condition] Includes unsubscribe/spam report metrics (`unsubscribes`, `spam_reports`) and can be aggregated by campaign (`int_hubspot__email_aggregate_status_change`, `hubspot__campaign_performance_analysis`).\n  - Sub-criterion: [1 Point | Condition] Has contact email interaction metrics (`email_open_rate`, `email_click_through_rate`, `last_engagement_date`) for recency and frequency assessment (`hubspot__contact_engagement_analysis`, `int_hubspot__contact_lifecycle_metrics`).\n  - Sub-criterion: [1 Point | Condition] Has proxy features related to deals/purchases (`total_associated_deals`, `total_closed_deals`, `total_deal_value`, `avg_deal_value`, `last_deal_closed_date`), with granularity of \"one row per contact\" (`hubspot__contact_engagement_analysis`).\n  - Sub-criterion: [1 Point | Condition] Has email attribution metrics (`email_attributed_deals`, `email_attributed_revenue`, `revenue_per_email_sent`, `avg_attribution_window_days`) to support evaluation of the email-to-deal relationship (`hubspot__contact_engagement_analysis`, `int_hubspot__email_marketing_attribution`).\n  - Sub-criterion: [1 Point | Condition] Has contact segmentation/scoring (`overall_engagement_score`, `engagement_tier`, `final_engagement_classification`, `customer_value_segment`, `mql_classification`, `comprehensive_engagement_score`) for audience prioritization (`hubspot__contact_engagement_analysis`).\n  - Sub-criterion: [1 Point | Condition] Has campaign-level performance and quality metrics (`open_rate`, `click_through_rate`, `attributed_revenue`, `lead_conversion_rate`, `avg_days_to_conversion`, `unsubscribe_rate`, `bounce_rate`, `delivery_rate`, `campaign_performance_tier`, `roi_tier`) (`hubspot__campaign_performance_analysis`).\n  - Sub-criterion: [1 Point | Condition] Has a send time field to support frequency control/quiet windows (`email_send_timestamp`), and can apply time constraints by contact/campaign (`hubspot__email_sends` / `hubspot__email_event_sent`).\n\n### Criterion 1.2 (Max 6 Points): Metric/Feature Definitions\n- Path 1.2.A (6 Points): Clear Definitions\n  - Sub-criterion: [1 Point | Condition] Each output item contains at least two types of elements (by whom/time range or alignment/statistical or aggregation rules), e.g., \"`contact_id` + `attribution_window_days`\".\n  - Sub-criterion: [1 Point | Condition] Provides columns or logical explanations for default handling of missing/abnormal/duplicate/negative values/future dates (e.g., deletion flag `is_contact_deleted`, unsubscribe/spam report as a compliance suppression signal).\n  - Sub-criterion: [1 Point | Condition] Consistent definitions across output items (e.g., `open_rate`, `click_through_rate` are consistent for both campaign and contact granularities).\n  - Sub-criterion: [1 Point | Condition] Presence of replaceable \"recency/frequency\" fields (e.g., `last_engagement_date` + `total_emails_sent`) to cover behavioral strength assessment.\n  - Sub-criterion: [1 Point | Condition] Presence of replaceable \"value/conversion\" fields (e.g., `total_deal_value`, `email_attributed_revenue`) to cover revenue assessment.\n  - Sub-criterion: [1 Point | Condition] Presence of replaceable \"compliance/suppression\" fields (e.g., `was_unsubscribed`, `spam_reports`, `is_contact_deleted`).\n\n## Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n\n### Criterion 2.1 (Max 2 Points): Format/Structure Check\n- Path 2.1.A (2 Points): Minimum Four Elements of a Model\n  - Sub-criterion: [1 Point | Condition] The model identifier (`name`) is explicitly listed in `gold.yaml`, and upstream dependencies (`source_models`) are completely listed in `gold.yaml`.\n  - Sub-criterion: [1 Point | Condition] Field definitions (`columns`) are retrievable and semantic in `gold.yaml`.\n\n### Criterion 2.2 (Max 2 Points): Dependency Closure\n- Path 2.2.A (2 Points): Buildable Dependency Graph\n  - Sub-criterion: [1 Point | Condition] No dangling references (all `source_models` can be found in `gold.yaml` or `stg_*`), no circular/self-references (dependencies are layered and directed).\n  - Sub-criterion: [1 Point | Condition] External sources (staging) have clear pointers and boundary descriptions (`stg_hubspot__*` series).\n\n## Requirement III: Design Quality (3 Points)\n\n### Criterion 3.1 (Max 1 Point): Clear Naming\n- Sub-criterion: [1 Point | Condition] Names are semantic and avoid ambiguity (e.g., `hubspot__contact_engagement_analysis` clearly expresses analytical semantics).\n\n### Criterion 3.2 (Max 2 Points): Reasonable Layering\n- Sub-criterion: [1 Point | Condition] Layer responsibilities are clear (staging→intermediate→marketing/sales/service→marts), and model dependencies are distinct.\n- Sub-criterion: [1 Point | Condition] Business logic is centralized (e.g., email attribution and engagement are presented in dedicated model sets), avoiding \"God tables\" and disorganized scattering.\n\n### Scoring Notes\n- Each sub-criterion is scored as 1/0; no evidence → 0 points.\n- `ALLOW_MULTI_MODEL=YES`."}
{"id": "dacomp-de-arch-011", "question": "Recent sprints appear to be completed, but there is a significant gap between commitment and actual completion. Additionally, mid-sprint additions/removals and priority changes are very frequent, affecting delivery stability. Please provide a commitment stability assessment for each sprint, broken down by project and team:\n\n*   The deviation between the initial commitment vs. final completion.\n*   The time window when changes are most concentrated.\n*   Whether waiting time caused by assignment/reassignment increases significantly around the peak period for changes.\n*   List the top outlier sprints along with the primary owners/component concentration.", "rubric": "# Sprint Commitment Stability Assessment Evaluation Criteria\n\n## [Total Score | 31 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (24 Points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n## Requirement I: Business Alignment & Semantic Accuracy (24 Points)\n\n### Standard 1.1 (Max 20 Points): Functional Point Coverage\n- Path 1.1.A (20 Points): **Complete Coverage**\n  - 1.1.A.1 【1 Point】Sprint entity and identifiers exist: `sprint_id`, `sprint_name`, and `board_id` all exist, and the grain is clearly defined (e.g., \"one row per sprint per day\"); missing any or ambiguous granularity → 0 points.\n  - 1.1.A.2 【1 Point】Sprint time window: `sprint_started_at`, `sprint_ended_at`, and `sprint_completed_at` fields all exist and are of timestamp type; missing any → 0 points.\n  - 1.1.A.3 【1 Point】Workload committed at the start: Existence of `story_points_committed` or `story_point_estimate_committed`, and `issues_committed`; missing metric or field → 0 points.\n  - 1.1.A.4 【1 Point】Workload completed at the end: Existence of `story_points_end` or `story_point_estimate_end`, and `resolved_sprint_issues`; missing any → 0 points.\n  - 1.1.A.5 【1 Point】Daily progress within the sprint: Includes at least `issues_completed`, `issues_remaining`, `story_points_completed`, `story_points_remaining`, `completion_ratio`; incomplete fields → 0 points.\n  - 1.1.A.6 【1 Point】Commitment vs. completion deviation is calculable: The same model contains both committed and end/completed columns, and covers `story_points_*` and `issues_*`; missing one category → 0 points.\n  - 1.1.A.7 【1 Point】Scope change volume: Existence of `scope_change_points`, `scope_change_issues`, or `burndown_variance`; 0 points if not present.\n  - 1.1.A.8 【1 Point】Change time window is locatable: Existence of `snapshot_date`/`day_number` or `timeline_progress_ratio`/`velocity_trend_3d` to mark periods of concentrated change; otherwise 0 points.\n  - 1.1.A.9 【1 Point】Assignment/reassignment waiting time: Existence of `any_assignment_duration_seconds`, `last_assignment_duration_seconds`, as well as timestamps `first_assigned_at`, `last_assigned_at`; missing any → 0 points.\n  - 1.1.A.10 【1 Point】Waiting time is associated with changes: Has daily records and can be joined/aligned with `scope_change_*` or timeline fields; otherwise 0 points.\n  - 1.1.A.11 【1 Point】Risk/health signals: At least one of the following exists: `sprint_health_score`, `sprint_risk_status`, `performance_index`, `velocity_category`; missing → 0 points.\n  - 1.1.A.12 【1 Point】Ranking of top anomalous sprints: Output includes columns for sorting criteria (e.g., `performance_index`, `sprint_health_score`, `burndown_variance`, `completion_ratio`); otherwise 0 points.\n  - 1.1.A.13 【1 Point】Assignee dimension: `assignee_user_id`, `assignee_name` exist and can be aggregated at the sprint level; otherwise 0 points.\n  - 1.1.A.14 【1 Point】Components dimension: `components` column exists and can be aggregated; otherwise 0 points.\n  - 1.1.A.15 【1 Point】Project/team dimension: `project_id`, `project_name` exist and can be associated with sprints (e.g., `sprints_participated`); otherwise 0 points.\n  - 1.1.A.16 【1 Point】Priority and type dimension: `current_priority`/`priority_id` and `issue_type` exist; missing → 0 points.\n  - 1.1.A.17 【1 Point】Estimated and actual work hours: `original_estimate_seconds`, `remaining_estimate_seconds`, `time_spent_seconds` exist; missing → 0 points.\n  - 1.1.A.18 【1 Point】Story points and story point estimates coexist: `story_points` and `story_point_estimate` both exist; missing either → 0 points.\n  - 1.1.A.19 【1 Point】Sprint active status: `is_sprint_active` and `sprint_phase` exist, and enum values are documented; otherwise 0 points.\n  - 1.1.A.20 【1 Point】Cross-model unique keys and time: `sprint_id`, `issue_id`, `date_day`/`snapshot_date` must be parsable in all relevant models; missing → 0 points.\n\n### Standard 1.2 (Max 4 Points): Metric/Feature Definitions\n- Path 1.2.A (4 Points): **Clear Definitions**\n  - 1.2.A.1 【1 Point】Each output item includes **entity + time**: e.g., `sprint_id+snapshot_date`, `issue_id+date_day`; missing one category → 0 points.\n  - 1.2.A.2 【1 Point】Missing/abnormal value tolerance: Existence of tests or constraints (e.g., numerical value ≥0, enum value set, time not later than the current date); no explicit tests → 0 points.\n  - 1.2.A.3 【1 Point】Consistent definitions across output items: `story_points*`, `story_point_estimate*`, `issues_*` have consistent definitions at daily and sprint levels (naming + units); inconsistent → 0 points.\n  - 1.2.A.4 【1 Point】Unified status/phase: `is_sprint_active`, `sprint_phase`, `current_status` are semantically consistent across different tables (same enum set); otherwise 0 points.\n\n## Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n\n### Standard 2.1 (Max 2 Points): Format/Structure Check\n- 2.1.A.1 【1 Point】Upstream dependencies (`source_models`) are fully declared, and the grain is clearly defined (e.g., \"One row per sprint per day\"); missing → 0 points.\n- 2.1.A.2 【1 Point】Field definitions (`columns`) list all columns with name+data_type; missing any → 0 points.\n\n### Standard 2.2 (Max 2 Points): Dependency Closure\n- 2.2.A.1 【1 Point】No circular/self-references: Dependency direction is stg → int → marts; cycle detected → 0 points.\n- 2.2.A.2 【1 Point】External sources are clear: All raw inputs come from `stg_*` and do not bypass the intermediate layer to go directly to marts; otherwise 0 points.\n\n## Requirement III: Design Quality (3 Points)\n\n### Standard 3.1 (Max 1 Point): Clear Naming\n- 3.1.1 【1 Point】Naming convention: snake_case, consistent units (*_seconds, *_days); the same concept is not used interchangeably (e.g., `story_points` vs `story_point_estimate` are clearly and consistently differentiated); otherwise 0 points.\n\n### Standard 3.2 (Max 2 Points): Reasonable Layering\n- 3.2.1 【1 Point】Business metrics are centralized: Sprint metrics are aggregated in the int and marts layers (e.g., `int_jira__sprint_performance_analytics`, `jira__daily_sprint_issue_history`, `jira__sprint_enhanced`); otherwise 0 points.\n- 3.2.2 【1 Point】Avoid \"God tables\": Modular cohesion (issue/project/user/sprint each form their own system), clear dependencies; 0 points if all dimensions are found mixed into a single table.\n\n## Scoring Notes\n- Each sub-standard is scored 1/0; **no evidence = 0 points**.\n- Hard constraints: consistent unique keys, naming conventions, unidirectional dependencies; any inconsistency → 0 points.\n- `ALLOW_MULTI_MODEL=YES`: Satisfaction across multiple models is allowed, but clear evidence of the fields/logic must be found in the model/contract."}
{"id": "dacomp-de-arch-012", "question": "In a recent review of our email marketing performance, we discovered significant fluctuations in the weekly \"open-to-click\" conversion rate for template combinations within the same channel and campaign type. This makes it difficult to maintain stable results from our resource allocation.\n\nPlease break down the data by `channel × campaign type × email template` and conduct a weekly funnel assessment for the last complete cycle to identify:\n\n*   Which combinations are both high-converting and stable.\n*   Which combinations have high conversion but are highly volatile (and should be deprioritized or re-verified with A/B testing).\n*   Which combinations have low volatility but weak click-through rates (making them suitable as baseline templates).\n\nAdditionally, please flag any anomalous weeks and provide possible structural reasons (e.g., template changes, changes in audience size, changes in delivery frequency).", "rubric": "# Email Operations Weekly Funnel (Channel × Activity Type × Template) Evaluation Criteria\n\n## [Total Score | 35 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (28 points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 points)\n- Requirement III: Design Quality (3 points)\n\n## Requirement I: Business Alignment & Semantic Accuracy (28 points)\n\n### Criterion 1.1 (Max 25 points): Feature Point Coverage\n- Path 1.1.A (25 points): Complete Coverage\n  - Sub-criterion 1.1.A.1: [1 point | Dimension] `channel` can be used for slicing and aggregation: The output **must** allow tracing `program_id`→`channel` (e.g., `marts.marketo__programs.channel`); `program_id` is non-null and joinable; otherwise 0 points.\n  - Sub-criterion 1.1.A.2: [1 point | Dimension] `campaign_type` is available: The fact/dimension table **must** provide `campaign_type` (evidence example: `stg_marketo__campaigns.campaign_type` and is joinable/referenceable in `email_sends`); otherwise 0 points.\n  - Sub-criterion 1.1.A.3: [1 point | Dimension] `email_template_id` is available: It is **referenceable** in both the fact layer and the funnel output (the column exists and has a consistent type); otherwise 0 points.\n  - Sub-criterion 1.1.A.4: [1 point | Dimension] `campaign_id` is available: It is **referenceable** in both the funnel/fact layer (evidence: the column appears in the funnel layer or can be mapped via business keys); otherwise 0 points.\n  - Sub-criterion 1.1.A.5: [1 point | Period] Supports **weekly granularity**: The output includes `analysis_type='weekly'` and a weekly identifier `period_identifier` (cannot be null); otherwise 0 points.\n  - Sub-criterion 1.1.A.6: [1 point | Time Alignment] **Consistent week definition**: Calculations must uniformly use `DATE_TRUNC('week', <timestamp>)` (or an equivalent function) and the timezone must be declared in the documentation; if a mix of calendar weeks/ISO weeks or inconsistent timezones is found, 0 points.\n  - Sub-criterion 1.1.A.7: [1 point | Metric] `period_emails_sent` weekly sent volume: The field exists, is non-negative (≥0), and is an integer type; otherwise 0 points.\n  - Sub-criterion 1.1.A.8: [1 point | Metric] `period_emails_delivered` weekly delivered volume: The field exists, is non-negative, and is an integer; otherwise 0 points.\n  - Sub-criterion 1.1.A.9: [1 point | Metric] `period_emails_opened` weekly open volume: The field exists, is non-negative, and is an integer; otherwise 0 points.\n  - Sub-criterion 1.1.A.10: [1 point | Metric] `period_emails_clicked` weekly click volume: The field exists, is non-negative, and is an integer; otherwise 0 points.\n  - Sub-criterion 1.1.A.11: [1 point | Metric] `period_unique_recipients` weekly unique recipients: The field exists, is non-negative, and is an integer; otherwise 0 points.\n  - Sub-criterion 1.1.A.12: [1 point | Metric] `period_unique_opened` weekly unique opens: The field exists, is non-negative, and is an integer; otherwise 0 points.\n  - Sub-criterion 1.1.A.13: [1 point | Metric] `period_unique_clicked` weekly unique clicks: The field exists, is non-negative, and is an integer; otherwise 0 points.\n  - Sub-criterion 1.1.A.14: [1 point | Supplemental] `total_open_actions"}
{"id": "dacomp-de-arch-013", "question": "Over the past four fiscal periods, the cash flow of some subsidiaries has turned from positive to negative, but the finance team has differing opinions on the cause. I need you to clearly break down the main reasons for this `negative cash flow`: Is it due to slower collections (worsening AR aging), gross margin erosion (abnormal expense structures at the department/location level), or a cash bleed caused by accelerated payments to suppliers? I require a list of the top 5 at-risk entities at the subsidiary level. Then, at a granular department-location-month level, provide the contribution percentage for each main cause and define clear corrective actions (collection priorities, specific points for expense control, and recommendations for supplier renegotiations). Furthermore, all of this must be calculated directly from our existing data warehouse.", "rubric": "# Root Cause Analysis of Negative Cash Flow: Evaluation Criteria\n\n## [Total Score | 27 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (20 points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 points)\n- Requirement III: Design Quality (3 points)\n\n## Requirement I: Business Alignment & Semantic Accuracy (20 points)\n\n### Criterion 1.1 (Max 17 points): Functional Point Coverage\n- Path 1.1.A (17 points): **Complete Coverage**\n  - 1.1.A.1 [1 point] Subsidiary Dimension: Must include `subsidiary_id`, `subsidiary_name`, and `subsidiary_full_name`; any missing → 0 points. Evidence: `int_netsuite2__transaction_details`, `netsuite2_financial_dashboard`.\n  - 1.1.A.2 [1 point] Financial Period Alignment: Must have `accounting_period_id` and `accounting_period_ending`/`name`, or `analysis_month`/`performance_month`; any missing → 0 points.\n  - 1.1.A.3 [1 point] Four Cash Flow Categories: `operating_cash_flow`, `investing_cash_flow`, `financing_cash_flow`, and `net_cash_flow` must all be present, and inflow/outflow must be distinguishable; any category missing → 0 points.\n  - 1.1.A.4 [1 point] Four-Period Window: A period or month granularity field must exist to support fetching the last 4 periods; if granularity cannot be aligned → 0 points.\n  - 1.1.A.5 [1 point] AR Aging Buckets: Must fully cover all six buckets from `current_amount` to `over_180_days_amount`; any missing → 0 points.\n  - 1.1.A.6 [1 point] AR Percentage and Deterioration: Must include `overdue_amount`, `overdue_percentage`, `weighted_average_days_outstanding`, and `customer_risk_score`; any missing → 0 points.\n  - 1.1.A.7 [1 point] Gross Profit Chain: Must include revenue, cost, gross profit, and gross margin (`total_revenue`, `total_cogs`, `gross_profit`, `gross_margin_pct`/`gross_margin_ratio`); any missing → 0 points.\n  - 1.1.A.8 [1 point] Expense Scope: Must have `revenue_to_expense_ratio` or `converted_amount` + account classification; missing → 0 points.\n  - 1.1.A.9 [1 point] Department Dimension: `department_id`, `department_name`, and `department_full_name` must be present; any missing → 0 points.\n  - 1.1.A.10 [1 point] Location Dimension: `location_id`, `location_name`, and `location_full_name` must be present; any missing → 0 points.\n  - 1.1.A.11 [1 point] Monthly Granularity: Must support \"department-location-month\" via `performance_month`/`analysis_month`/`accounting_period_*`; any missing → 0 points.\n  - 1.1.A.12 [1 point] Vendor Payment Delay: Must include `avg_payment_delay_days`, `overdue_payment_percentage`, and `outstanding_balance`/`total_payables`; any missing → 0 points.\n  - 1.1.A.13 [1 point] Vendor Dimension: `vendor_id`, `vendor_name`, and `vendor_category_name` must all be present; any missing → 0 points.\n  - 1.1.A.14 [1 point] Contribution Percentage: Must have at least one type of ratio/percentage (e.g., `gross_margin_percentage`, `overdue_percentage`, `spend_concentration_ratio`); otherwise 0 points.\n  - 1.1.A.15 [1 point] Top 5 Subsidiary Risks: A risk column sortable by subsidiary must exist (`customer_risk_score`, `vendor_risk_score`, or `overdue_amount`/`avg_payment_delay`); otherwise 0 points.\n  - 1.1.A.16 [1 point] Corrective Actions: Must have `recommended_action` or an equivalent field (`collection_priority`, `payment_recommendation`, `improvement_recommendation`); otherwise 0 points.\n  - 1.1.A.17 [1 point] Multi-Model Synergy: Cash flow/AR/gross profit/expense/vendor models must all exist and be joinable by at least `subsidiary_id` + `accounting_period_id`/`month`; otherwise 0 points.\n\n- Path 1.1.B (3 points): **Basic Coverage**\n  - 1.1.B.1 [1 point] Entity Coverage: Must cover at least 2 types from subsidiary (subsidiary_*) + entity (customer/vendor/department/location); otherwise 0 points.\n  - 1.1.B.2 [1 point] Time Coverage: Must cover at least 2 items from `accounting_period_*` or `analysis_month/performance_month`; otherwise 0 points.\n  - 1.1.B.3 [1 point] Aggregation Coverage: Metrics for at least 2 of the following categories must exist: cash flow/AR/gross profit/expenses/vendor payments; otherwise 0 points.\n\n### Criterion 1.2 (Max 3 points): Metric/Feature Scope\n- Path 1.2.A (3 points): **Clear Scope**\n  - 1.2.A.1 [1 point] Each output item must include an entity + time (e.g., `subsidiary_id+accounting_period_id`); any missing → 0 points.\n  - 1.2.A.2 [1 point] Must handle exceptions/missing values/negative values (tests: not_null, unique, >=0 or data_quality_metrics); no tests → 0 points.\n  - 1.2.A.3 [1 point] Consistent scope across outputs: `period` vs `month` naming is consistent; definitions of cash flow/AR/profit/expense fields do not conflict across models; conflicts → 0 points.\n\n## Requirement II: Technical Feasibility & Structural Completeness (4 points)\n\n### Criterion 2.1 (Max 2 points): Format/Structure Check\n- 2.1.A.1 [1 point] Grain definition must be explicit (e.g., \"one row per subsidiary per month\"); missing → 0 points.\n- 2.1.A.2 [1 point] All fields must be declared with `name`+`data_type` under `columns`; missing → 0 points.\n\n### Criterion 2.2 (Max 2 points): Dependency Closure\n- 2.2.A.1 [1 point] No dangling references: All `stg/int/netsuite2_*` models pointed to by `source_models` must exist within the contract; missing → 0 points.\n- 2.2.A.2 [1 point] Clear external source boundaries: All raw inputs must come from `stg_netsuite2__*` or `stg_netsuite__*`; direct cross-references to marts are forbidden; non-compliant → 0 points.\n\n## Requirement III: Design Quality (3 points)\n\n### Criterion 3.1 (Max 2 points): Clear Naming\n- 3.1.1 [1 point] Semantic field names (e.g., `operating_cash_flow`, `overdue_amount`), avoiding abbreviations and jargon.\n- 3.1.2 [1 point] Consistent naming conventions: `*_id`, `*_amount`, `*_percentage`, `*_ratio`, `*_month`, etc., should be uniform; the same concept should not use mixed names across models (e.g., `gross_margin_pct` vs `gross_margin_ratio` must be one-to-one).\n\n### Criterion 3.2 (Max 1 point): Reasonable Layering\n- 3.2.1 [1 point] Clear layering: stg (raw) → int (processed) → netsuite2_* (mart); metrics for the same theme are not scattered, and cross-theme connections are made via keys (`subsidiary_id`/`accounting_period_id`); otherwise 0 points.\n\n## Scoring Notes\n- Each sub-criterion is strictly scored as 1/0; **no evidence = 0 points**.\n- Any missing fields, missing tests, or inconsistent scope across models → results in 0 points.\n- `ALLOW_MULTI_MODEL=YES`: Cross-model fulfillment is allowed, but **explicit field/rule evidence** must be found in the model or contract."}
{"id": "dacomp-de-arch-014", "question": "We are currently distributing each survey in parallel across multiple channels (e.g., email, anonymous links, social communities). Please help me perform a weekly attribution analysis on the responses that are \"actually completed and meet quality standards,\" broken down by distribution channel and contact source directory dimensions: within the same survey (`survey_id`), when a contact receives multiple distributions (`distribution_id`) or enters through different channels, attribute the contact's final \"qualified completion\" only to the first channel and distribution they interacted with. If a contact has multiple accounts merged and deduplicated in the directory (by the same email address), perform the attribution deduplication at the email level. Ultimately, for each survey, each week, and each channel, I need the \"number of qualified completions,\" \"channel market share,\" and \"channel efficiency score ranking.\" I also need a view stratified by directory (`directory_id`) to see the contribution of qualified completions from different directories across each channel, so that we can decide how to allocate next week's budget across channels and directories.", "rubric": "# Weekly Channel Attribution (Qualtrics) Evaluation Criteria\n\n## [Total Score | 36 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (29 Points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n## Requirement I: Business Alignment & Semantic Accuracy (29 Points)\n\n### Criterion 1.1 (Max 26 Points): Functional Point Coverage\n- Path 1.1.A (26 Points): **Complete Coverage**\n  - Sub-criterion: Survey primary key exists [1 Point | Condition] Contains column `survey_id`\n  - Sub-criterion: Distribution primary key exists [1 Point | Condition] Contains column `distribution_id`\n  - Sub-criterion: Channel enumeration is valid [1 Point | Condition] Contains column `distribution_channel` with a value range of 'email','web','mobile','social','sms','qr','anonymous'\n  - Sub-criterion: Contact and email identifiers exist [1 Point | Condition] Contains columns `contact_id`, `email`\n  - Sub-criterion: Directory dimension exists [1 Point | Condition] Contains column `directory_id`\n  - Sub-criterion: Response completion flag exists (completed vs. not completed) [1 Point | Condition] Contains column `is_finished`\n  - Sub-criterion: Response quality qualification flag exists [1 Point | Condition] Contains column `include_in_analysis` (qualified)\n  - Sub-criterion: Response progress and duration exist [1 Point | Condition] Contains columns `progress`, `duration_in_seconds`\n  - Sub-criterion: Response date (daily granularity) exists [1 Point | Condition] Contains column `recorded_date` or `date_day` (mart)\n  - Sub-criterion: Weekly dimension can be aggregated from daily data [1 Point | Condition] Column `date_day` can derive `week_start` (derived dimensions are scorable)\n  - Sub-criterion: Daily \"completed response count\" by channel exists [1 Point | Condition] Completed count columns for email/smsinvite/qr/gl/social/anonymous exist\n  - Sub-criterion: Channel total responses and completion rate exist [1 Point | Condition] Contains columns `total_responses`, `completed_responses`, `completion_rate`\n  - Sub-criterion: Channel market share exists [1 Point | Condition] Contains column `market_share`\n  - Sub-criterion: Channel efficiency score exists [1 Point | Condition] Contains column `efficiency_score`\n  - Sub-criterion: Channel efficiency/completion rank exists [1 Point | Condition] Contains columns `efficiency_rank`, `completion_rank`\n  - Sub-criterion: Number of channels used by the survey exists [1 Point | Condition] Contains column `unique_channels_used` (or equivalent)\n  - Sub-criterion: Contact distribution time events exist (sent/opened/started/completed) [1 Point | Condition] Contains columns `sent_at`, `opened_at`, `response_started_at`, `response_completed_at`\n  - Sub-criterion: Email normalization and validation exist [1 Point | Condition] Column `recipient_email` is standardized/format-validated\n  - Sub-criterion: Directory deduplication flag exists (by email, etc.) [1 Point | Condition] Contains column `is_deduped_on_email` or similar\n  - Sub-criterion: Channel contribution by directory is mappable [1 Point | Condition] Dimension keys of the directory model and the channel/response model can be joined (`directory_id`×`distribution_channel`×`date_day`);\n  - Sub-criterion: Survey-level channel performance exists [1 Point | Condition] `int_qualtrics__survey_performance` contains channel evaluation logic\n  - Sub-criterion: Screened-out count exists [1 Point | Condition] Contains column `current_count_surveys_surveyscreenedout`;\n  - Sub-criterion: User engagement model exists (for unique participant count) [1 Point | Condition] `int_qualtrics__user_engagement` model and unique participant column exist;\n  - Sub-criterion: Survey metadata exists (category/type) [1 Point | Condition] Contains columns `project_category`, `project_type` or equivalent\n- Path 1.1.B (1 Point): **Basic Coverage**\n  - Sub-criterion: Covers at least two of the three main categories: entity, time, and aggregation [1 Point | Condition] At least two of the following exist: dimension keys (e.g., `survey_id`, `distribution_channel`), time (daily or weekly), and aggregate metrics (e.g., `completion_rate`, `market_share`)\n\n### Criterion 1.2 (Max 3 Points): Metric/Feature Definition\n- Path 1.2.A (3 Points): **Clear Definition**\n  - Sub-criterion: Each output item includes at least two types of elements (by whom/time range or alignment/statistical or aggregation rule) [1 Point | Condition] Grain + dimension keys (e.g., `survey_id`, `distribution_channel`, `directory_id`) coexist with time (daily/weekly) or aggregate metrics\n  - Sub-criterion: Default handling for missing/abnormal/duplicate/negative values/future times is provided [1 Point | Condition] Data contract includes tests/constraints or field descriptions that reflect default handling strategies (e.g., `completion_rate` BETWEEN 0 AND 1; `email` validation)\n  - Sub-criterion: Definitions are consistent and not contradictory across output items [1 Point | Condition] The same metrics are defined consistently (e.g., `completion_rate`, `market_share` are consistent across all models)\n\n## Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n\n### Criterion 2.1 (Max 2 Points): Format/Structure Check\n- Path 2.1.A (2 Points): **Minimum Four Model Elements**\n  - Sub-criterion: Has a model identifier (name) [1 Point | Condition] Each model definition includes a `name`\n  - Sub-criterion: Has a granularity definition (grain) [1 Point | Condition] Each model defines a `grain` (e.g., `qualtrics__daily_breakdown`: one row per day)\n\n### Criterion 2.2 (Max 2 Points): Dependency Closure\n- Path 2.2.A (2 Points): **Dependency Graph is Buildable**\n  - Sub-criterion: No dangling references [1 Point | Condition] All `source_models` can be resolved to specific models\n  - Sub-criterion: No circular/self-references [1 Point | Condition] The dependency graph is acyclic\n\n## Requirement III: Design Quality (3 Points)\n\n### Criterion 3.1 (Max 1 Point): Clear Naming\n- Sub-criterion: Names are semantic, avoiding jargon/ambiguity [1 Point | Condition] Model/field names intuitively express business meaning (e.g., `distribution_channel`, `completion_rate`, `efficiency_score`), and naming conventions are consistent (e.g., `stg_`/`int_`/`qualtrics__*` prefixes for layering)\n\n### Criterion 3.2 (Max 2 Points): Reasonable Layering\n- Sub-criterion: Layer responsibilities are clear (raw/intermediate/mart) [1 Point | Condition] The `raw/staging`→`int_qualtrics__*`→`qualtrics__*` system is complete; key business definitions are centralized in dedicated models in the intermediate and mart layers (e.g., `response_quality`, `channel_performance`, `daily_breakdown`)\n- Sub-criterion: Avoids \"God tables,\" modules are cohesive, and dependencies are clear [1 Point | Condition] Models are appropriately partitioned, with no oversized, mixed-purpose tables, and dependencies are unidirectional and clear\n\n### Scoring Notes\n- Each sub-criterion is scored as 1 or 0; no evidence → 0 points.\n- ALLOW_MULTI_MODEL=YES."}
{"id": "dacomp-de-arch-015", "question": "We want to identify a group of customers who are \"declining and may drag down cash flow\": specifically, those whose revenue has declined quarter-over-quarter in the past quarter, whose overdue accounts receivable amount is increasing, and whose historical payment rate is below 70%.\n\nPlease provide a complete, customer-level profile for this group, along with system-recommended follow-up actions. The output must be a directly actionable checklist for collaborative efforts between the Sales and Finance teams.", "rubric": "# Evaluation Criteria for Declining and Cash-Flow-Dragging Customer Identification and Action List\n\n## [Total Score | 36 Points] Three Fixed Requirements\n- Requirement I: Business Alignment and Semantic Accuracy (29 points)\n- Requirement II: Technical Feasibility and Structural Completeness (4 points)\n- Requirement III: Design Quality (3 points)\n\n## Requirement I: Business Alignment and Semantic Accuracy (29 points)\n\n### Criterion 1.1 (Max 26 points): Functional Point Coverage\n- Path 1.1.A (26 points): Complete Coverage\n  - Sub-criterion [1 point | Condition]: Customer-level granularity output exists (e.g., `quickbooks__customer_analytics` granularity is \"one row per customer\").\n  - Sub-criterion [1 point | Condition]: Complete set of quarterly dimension fields (e.g., `revenue_quarter`, `revenue_quarter_start` or `transaction_quarter`, `transaction_quarter_start`).\n  - Sub-criterion [1 point | Condition]: Quarter-over-Quarter (QoQ) revenue comparison fields are present (e.g., `revenue_current_quarter`, `revenue_prev_quarter` or `revenue_growth_qoq_pct`).\n  - Sub-criterion [1 point | Condition]: Customer-level historical payment rate field (e.g., `payment_rate_percentage` or `customer_collection_rate`).\n  - Sub-criterion [1 point | Condition]: Outstanding balance field (e.g., `outstanding_balance`, at customer or invoice level).\n  - Sub-criterion [1 point | Condition]: AR aging bucket field (e.g., `aging_bucket` or equivalent overdue status).\n  - Sub-criterion [1 point | Condition]: Overdue identification field (e.g., `collection_status` or `cash_flow_status` containing \"Overdue\"/\"Past Due\").\n  - Sub-criterion [1 point | Condition]: Time-series granularity supports \"increase in overdue amount\" determination (monthly/quarterly aligned fields, e.g., `revenue_month_start` or `transaction_month_start`).\n  - Sub-criterion [1 point | Condition]: Cash flow direction/amount fields (e.g., `cash_flow_direction`, `net_cash_flow_amount`, `net_outstanding_amount`).\n  - Sub-criterion [1 point | Condition]: Expected cash flow timing field (e.g., `expected_cash_date`).\n  - Sub-criterion [1 point | Condition]: Cash flow risk/health field (e.g., `liquidity_risk_level` or `cash_flow_health`).\n  - Sub-criterion [1 point | Condition]: Recommended action field (either `recommended_action` on the customer side, or `management_recommendation` on the capital side, satisfies the condition).\n  - Sub-criterion [1 point | Condition]: Basic customer profile information fields (e.g., `customer_name`/`company_name`/`display_name`).\n  - Sub-criterion [1 point | Condition]: Customer lifecycle/activity fields (e.g., `lifecycle_stage`, `activity_status`).\n  - Sub-criterion [1 point | Condition]: Customer value/behavior segmentation fields (e.g., `customer_value_segment`, `payment_behavior`).\n  - Sub-criterion [1 point | Condition]: Vendor payment-related metrics (for observing cash pressure, e.g., `vendor_on_time_payment_rate` or `paid_amount`).\n  - Sub-criterion [1 point | Condition]: Expense/spending metrics (e.g., `total_spend`/`monthly_total_spend`).\n  - Sub-criterion [1 point | Condition]: Gross profit/gross margin metrics (e.g., `gross_profit`, `invoice_gross_margin_pct` or `monthly_gross_profit`).\n  - Sub-criterion [1 point | Condition]: All elements required for contribution percentage are present (both \"customer revenue metric\" + \"organization total revenue metric\" are obtainable for the same period, e.g., customer-side `revenue_current_quarter` and organization-side `total_revenue`/monthly aggregates can be rolled up).\n  - Sub-criterion [1 point | Condition]: Subsidiary/department dimension (e.g., `department_id` or equivalent, including dimension tables like `stg_quickbooks__department`).\n  - Sub-criterion [1 point | Condition]: Invoice collection status field (e.g., `collected_amount`/`collection_status`).\n  - Sub-criterion [1 point | Condition]: Consistent join keys across models (`customer_id` can be used to join across `revenue`/`cashflow`/`customer_analytics`).\n  - Sub-criterion [1 point | Condition]: Actionable identifier fields for the action list (`customer_id` and company/display name for distribution).\n  - Sub-criterion [1 point | Condition]: Basis for risk/priority ranking (e.g., `business_health_score` or `liquidity_risk_level`).\n\n### Criterion 1.2 (Max 3 points): Metric/Feature Definition\n- Path 1.2.A (3 points): Clear Definition\n  - Sub-criterion [1 point | Condition]: Each output item includes at least two types of elements (by whom + time range/alignment, or statistical/aggregation rules), and is consistent with the model's grain (Example: grain definitions for each model).\n  - Sub-criterion [1 point | Condition]: Default handling or constraints are specified for missing/abnormal/duplicate/negative/future time values (column-level `constraints`/data type constraints can be identified as test definitions).\n  - Sub-criterion [1 point | Condition]: Consistent definitions across output items (e.g., \"overdue/uncollected/payment rate/quarter\" have the same meaning in different models, and field naming and descriptions are consistent).\n\n## Requirement II: Technical Feasibility and Structural Completeness (4 points)\n\n### Criterion 2.1 (Max 2 points): Format/Structure Check\n- Path 2.1.A (2 points): Four Minimum Model Elements\n  - Sub-criterion [1 point | Condition]: Has a model identifier (`name`), upstream dependencies (`source_models`), and a grain definition (`grain`).\n  - Sub-criterion [1 point | Condition]: Has field definitions + types (`columns`).\n\n### Criterion 2.2 (Max 2 points): Dependency Closure\n- Path 2.2.A (2 points): Dependency Graph Can Be Built\n  - Sub-criterion [1 point | Condition]: No dangling references, no circular/self-references (dependency graph is a Directed Acyclic Graph - DAG).\n  - Sub-criterion [1 point | Condition]: External sources have clear pointers and boundary descriptions (only references models within the `stg_`/`int_`/`marts_` scope or `raw_*` as sources, with clear boundaries).\n\n## Requirement III: Design Quality (3 points)\n\n### Criterion 3.1 (Max 1 point): Clear Naming\n- Sub-criterion [1 point | Condition]: Semantic naming (prefixes like `quickbooks__`/`int_quickbooks__`/`stg_quickbooks__` are consistent with the entity), consistent abbreviations/naming conventions (double underscores, consistent layer prefixes).\n\n### Criterion 3.2 (Max 2 points): Reasonable Layering\n- Sub-criterion [1 point | Condition]: Clear layer responsibilities (`staging`→`intermediate`→`marts`, with definitions established in the intermediate/marts layers).\n- Sub-criterion [1 point | Condition]: Avoids \"God tables\"; modules are cohesive with clear dependencies (split by domain, e.g., customer/vendor/account).\n\n### Scoring Notes\n- Each sub-criterion is scored as 1 or 0; no evidence → 0 points.\n- ALLOW_MULTI_MODEL=YES."}
{"id": "dacomp-de-arch-016", "question": "How stable has the staffing for our \"Manager and above\" positions been across all business units over the last four quarters?\n\nProvide me with a quarterly ranking that compares across organizational units and can be drilled down to the position/role level. The report should include:\n\n*   Position fill rate\n*   Position utilization rate\n*   Quarterly turnover rate\n*   90-day and one-year retention rates\n*   Average tenure\n*   The proportion of fast-track promotions within 12 months for the same position\n\nAdditionally, please flag the \"high-risk positions\" (high turnover + low retention + low utilization) and, in conjunction with the organization's overall performance ratings, provide recommended actions, such as accelerating internal mobility or suspending new headcount.", "rubric": "# Workday HR Stability Quarterly Leaderboard Evaluation Criteria\n\n## [Total Score | 33 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (26 Points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n## Requirement I: Business Alignment & Semantic Accuracy (26 Points)\n\n### Criterion 1.1 (Max 22 Points): Functional Point Coverage\n- Path 1.1.A (22 Points): **Complete Coverage**\n  - Sub-criterion: [1 Point | Condition] Organizational dimension (department/business unit) is comparable: Organizational granularity and identifier columns/grain exist (e.g., `workday__organization_overview: organization_id, organization_name`, `workday__organization_performance: organization_id`).\n  - Sub-criterion: [1 Point | Condition] Job profile/position drill-down: Position granularity and associations exist (`workday__position_overview: position_id` with `organization_id`; `workday__job_overview: job_profile_id`).\n  - Sub-criterion: [1 Point | Condition] Position fill rate: Organization-level `position_fill_rate` (`workday__organization_overview: position_fill_rate`), and position occupancy status (`workday__position_overview: position_occupancy_status`).\n  - Sub-criterion: [1 Point | Condition] Position utilization rate: Enterprise-level `position_utilization_rate` (`workday__workforce_analytics: position_utilization_rate`), or position utilization-related metrics (`current_workers_assigned/total_workers_ever_assigned`).\n  - Sub-criterion: [1 Point | Condition] Quarterly turnover rate: Organization-level `annual_turnover_rate` (`workday__organization_overview: annual_turnover_rate`), position-level `position_turnover_rate` (`workday__position_overview: position_turnover_rate`).\n  - Sub-criterion: [1 Point | Condition] 90-day retention rate: Contains `retained_90_days` with a clear source (`workday__employee_overview: retained_90_days`; `int_workday__employee_lifecycle` provides the definition).\n  - Sub-criterion: [1 Point | Condition] One-year retention rate: `retained_1_year` (`workday__employee_overview: retained_1_year`; `int_workday__employee_lifecycle`).\n  - Sub-criterion: [1 Point | Condition] Average tenure: `avg_tenure_years` (`workday__organization_overview: avg_current_tenure_years/avg_former_tenure_years`; `workday__employee_overview: tenure_years`).\n  - Sub-criterion: [1 Point | Condition] Proportion of fast promotions within the same job profile within 12 months: `fast_promotions` or `fast_track_employees` / current employees (`workday__employee_overview: fast_promotions`; `workday__job_overview: fast_track_employees_count`).\n  - Sub-criterion: [1 Point | Condition] High-risk position identifier: A combination of high turnover + low retention + low utilization, with position or organization risk classifications present (`workday__position_overview: position_performance_category/position_efficiency_score`; `workday__organization_performance: organization_risk_level`).\n  - Sub-criterion: [1 Point | Condition] Organization overall performance classification: `performance_category` (`workday__organization_overview: performance_category`, `workday__organization_performance: performance_category`).\n  - Sub-criterion: [1 Point | Condition] Recommended action output: `recommended_action/management_recommendation` (`workday__organization_performance: recommended_action`; `workday__organization_overview: management_recommendation`).\n  - Sub-criterion: [1 Point | Condition] Identification of manager and above positions: Management level definition is available (`workday__employee_overview: highest_management_level_reached`, `management_positions_held`; `workday__position_overview: management_level/management_level_numeric`).\n  - Sub-criterion: [1 Point | Condition] Time dimension for the quarterly leaderboard: A quarter/analysis date field exists to support quarterly statistics (`workday__employee_overview: hire_quarter_cohort`; `workday__workforce_analytics: analysis_date`).\n  - Sub-criterion: [1 Point | Condition] Employee risk stratification: `churn_risk_category` (`workday__employee_overview: churn_risk_category`).\n  - Sub-criterion: [1 Point | Condition] Employee retention milestones: `retained_30_days/90_days/1_year/3_years` (`workday__employee_overview`).\n  - Sub-criterion: [1 Point | Condition] Job profile/organization layer alignment: `job_profile` and `position` correspond to an organization (`workday__position_overview: organization_id`; `workday__job_overview: organizations_using_job_count/primary_organization_id`).\n  - Sub-criterion: [1 Point | Condition] Department size and management ratio: `organization_size_category`, `management_ratio` (`workday__organization_overview`).\n  - Sub-criterion: [1 Point | Condition] Employee value segmentation: `employee_value_segment` (`workday__employee_overview`), and aggregation at the organization/job profile level (`workday__organization_performance`, `workday__job_overview`).\n  - Sub-criterion: [1 Point | Condition] Working hours and workload: `default_weekly_hours/avg_weekly_hours` (`workday__employee_overview`, `workday__organization_overview`).\n  - Sub-criterion: [1 Point | Condition] Demographics and diversity: `age`, `ethnicity_codes`, `is_military_service` (`workday__employee_overview`; for aggregation at the organization/position level, see `workday__organization_overview`, `workday__position_overview`).\n\n- Path "}
{"id": "dacomp-de-arch-017", "question": "We are currently observing that the bottlenecks in our recruiting funnel vary across different departments and hiring managers—some positions receive many applications but progress slowly to the interview stage, while others have frequent interviews scheduled but low-quality scorecards, and the time-to-accept after an offer is extended is also quite long. Without changing the current data collection methods, please provide a quarterly funnel diagnosis for `'Job × Source Channel × Hiring Manager'`: calculate the conversion rates for three stages: `Application → First-round Interview`, `First-round Interview → Offer`, and `Offer → Hired`, as well as the average time spent in days for each stage. Simultaneously, identify the key bottlenecks causing delays (e.g., a high number of scheduled interviews but a low percentage of positive recommendations, or insufficient candidate-related activities in a given stage). Finally, output a list of the top lagging positions for the quarter along with corresponding actionable suggestions (e.g., optimize candidate selection for interviews, improve the completion rate of scorecards, increase the frequency of candidate touchpoints, etc.).", "rubric": "# Position × Source Channel × Hiring Manager Quarterly Funnel Diagnosis Evaluation Criteria (Strictly Numbered Version)\n\n## [Total Score | 30 Points] Three Fixed Requirements\n- Requirement I: Business Alignment and Semantic Accuracy (23 Points)\n- Requirement II: Technical Feasibility and Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n## Requirement I: Business Alignment and Semantic Accuracy (23 Points)\n\n### Criterion 1.1 (Max 20 Points): Feature Point Coverage\n- Path 1.1.A (20 Points): **Complete Coverage**\n  - Sub-criterion 1.1.A.1: [1 Point | Condition] Must have a position identifier field (`job_id` or `requisition_id`), which must be non-null and unique; `job_name` is optional, but if present, must map consistently to the ID.\n  - Sub-criterion 1.1.A.2: [1 Point | Condition] Must have a source identifier (`source_id`) and source type (`source_type`/`sourced_from_type`); the value domain must be stable (e.g., referral, job_board, internal, etc.); otherwise 0 points.\n  - Sub-criterion 1.1.A.3: [1 Point | Condition] Must have a hiring manager field (`hiring_managers` or an equivalent column) that supports aggregation for multiple hiring managers; 0 points if missing.\n  - Sub-criterion 1.1.A.4: [1 Point | Condition] Must have quarterly time dimensions: `application_year` and `application_quarter`; raw timestamps alone are not permitted.\n  - Sub-criterion 1.1.A.5: [1 Point | Condition] Must have department/parent department fields: `job_departments` and `job_parent_departments`; 0 points if either is missing.\n  - Sub-criterion 1.1.A.6: [1 Point | Condition"}
{"id": "dacomp-de-arch-018", "question": "This week, let's not just look at download numbers. I'm more concerned about \"new user quality\". Please create a weekly cohort table broken down by country, channel, platform version, and app version. Anchored by the 'week of first download', provide the 7-day activation rate (`active_devices/first_time_downloads`), 30-day sessions per user (`sessions/active_devices`), 7-day deletion rate (`deletions/first_time_downloads`), and crashes per 1k sessions. Also, compare net downloads (`installations−deletions`) and the redownload percentage. Help me identify the version/platform combinations with significantly low quality, list the top and bottom 10 problematic regions and sources, and advise me on whether we need to pause the campaign for any specific version.", "rubric": "# Apple Store Acquisition Quality Cohort Weekly Report Evaluation Criteria\n\n## [Total Score | 29 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (23 Points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n## Requirement I: Business Alignment & Semantic Accuracy (23 Points)\n\n### Standard 1.1 (Max 19 Points): Feature Point Coverage\n- Path 1.1.A (19 Points): **Complete Coverage**\n  - Sub-standard 1.1.A.1: [1 Point | Criterion] The model output **must explicitly contain** the `date_day` and `first_time_downloads` fields, and the field types must be correct (`date_day` as a date, `first_time_downloads` as a non-negative integer); otherwise, 0 points.\n  - Sub-standard 1.1.A.2: [1 Point | Criterion] The output **must contain both** the `territory_short` and `territory_long` fields, and their value ranges must conform to ISO country codes and standard country names; if one is missing or naming is inconsistent, 0 points.\n  - Sub-standard 1.1.A.3: [1 Point | Criterion] The `source_type` field must exist, and its value range must not exceed 10 preset channels (e.g., organic, referral, paid, etc.); if not specified, 0 points.\n  - Sub-standard 1.1.A.4: [1 Point | Criterion] `platform_version` must be present and in a string/version number format (e.g., iOS 17.0); otherwise, 0 points.\n  - Sub-standard 1.1.A.5: [1 Point | Criterion] `app_version` must be present and findable in `apple_store__app_version_report`; if missing, 0 points.\n  - Sub-standard 1.1.A.6: [1 Point | Criterion] The `active_devices` field must be a daily granularity metric and have ≥30 consecutive days of records; otherwise, 0 points.\n  - Sub-standard 1.1.A.7: [1 Point | Criterion] `sessions` must exist, be at daily granularity, and be dimensionally aligned with `active_devices`; otherwise, 0 points.\n  - Sub-standard 1.1.A.8: [1 Point | Criterion] `deletions` must exist, be at daily granularity, and have no negative values; otherwise, 0 points.\n  - Sub-standard 1.1.A.9: [1 Point | Criterion] `crashes` must exist, be at daily granularity, and support crash rate calculation in conjunction with `sessions`; otherwise, 0 points.\n  - Sub-standard 1.1.A.10: [1 Point | Criterion] `installations` must exist and be usable for net downloads (`installations - deletions`); if missing or naming does not conform, 0 points.\n  - Sub-standard 1.1.A.11: [1 Point | Criterion] `total_downloads` must exist, and its sum must be ≥ `first_time_downloads + redownloads`; otherwise, 0 points.\n  - Sub-standard 1.1.A.12: [1 Point | Criterion] `redownloads` must exist, and all values must be non-negative integers; otherwise, 0 points.\n  - Sub-standard 1.1.A.13: [1 Point | Criterion] Must be able to directly calculate **7-day active rate** (`7d_active_devices / 7d_first_time_downloads`) from the data, and the result must not have division-by-zero or negative value issues; otherwise, 0 points.\n  - Sub-standard 1.1.A.14: [1 Point | Criterion] Must be able to directly calculate **30-day sessions per user** (`30d_sessions / 30d_active_devices`) from the data, and the result must be reasonable; otherwise, 0 points.\n  - Sub-standard 1.1.A.15: [1 Point | Criterion] Must be able to directly calculate **7-day deletion rate** (`7d_deletions / 7d_first_time_downloads`) from the data, with no negative values; otherwise, 0 points.\n  - Sub-standard 1.1.A.16: [1 Point | Criterion] Must be able to directly calculate **crashes per 1000 sessions** (`1000 * crashes / sessions`), where sessions > 0; otherwise, 0 points.\n  - Sub-standard 1.1.A.17: [1 Point | Criterion] Must be able to obtain both **net downloads and redownload ratio** from the data simultaneously, with consistent definitions; otherwise, 0 points.\n  - Sub-standard 1.1.A.18: [1 Point | Criterion] `apple_store__platform_version_report` and `apple_store__app_version_report` must exist, and at least one field must be able to output the above metrics; otherwise, 0 points.\n  - Sub-standard 1.1.A.19: [1 Point | Criterion] `apple_store__territory_report` and `apple_store__source_type_report` must exist, and any metric must support top-10/bottom-10 sorting; otherwise, 0 points.\n\n- Path 1.1.B (1 Point): **Basic Coverage**\n  - Sub-standard 1.1.B.1: [1 Point | Criterion] At least `app_id` or `app_name` and `date_day` must exist, and include ≥1 core metric (e.g., `first_time_downloads` or `active_devices`); otherwise, 0 points.\n\n### Standard 1.2 (Max 4 Points): Metric/Feature Definition\n- Path 1.2.A (4 Points): **Clear Definition**\n  - Sub-standard 1.2.A.1: [1 Point | Criterion] The output must contain all three elements: **entity, time, and metric**; if any are missing, 0 points.\n  - Sub-standard 1.2.A.2: [1 Point | Criterion] The model documentation must explicitly state the logic for handling missing values, negative values, and future dates (e.g., `not_null`, `>=0`, fill strategy); if not specified, 0 points.\n  - Sub-standard 1.2.A.3: [1 Point | Criterion] The naming and meaning of the same metric must be consistent across all models; for example, if `sessions` means \"launches\" in one model and \"active users\" in another, 0 points.\n  - Sub-standard 1.2.A.4: [1 Point | Criterion] Dimension fields (e.g., `territory_short`, `source_type`, `platform_version`, `app_version`) must have a unified semantic meaning and must not use different definitions across different models; otherwise, 0 points.\n\n## Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n\n### Standard 2.1 (Max 2 Points): Format/Structure Check\n- Path 2.1.A (2 Points): **Model's Four Minimum Elements**\n  - Sub-standard 2.1.A.1: [1 Point | Criterion] The model must have a `name`, declare `source_models`, and this declaration must match the actual dependencies; if any are missing, 0 points.\n  - Sub-standard 2.1.A.2: [1 Point | Criterion] The `grain` must be clearly defined, and all `columns` (field name + type + description) must be listed; otherwise, 0 points.\n\n### Standard 2.2 (Max 2 Points): Dependency Closure\n- Path 2.2.A (2 Points): **Buildable Dependency Graph**\n  - Sub-standard 2.2.A.1: [1 Point | Criterion] All referenced models/dimension tables must be defined in the `data_contract`, and their field types must match exactly; if missing or inconsistent, 0 points.\n  - Sub-standard 2.2.A.2: [1 Point | Criterion] The dependency graph must not contain circular dependencies or self-references; otherwise, 0 points.\n\n## Requirement III: Design Quality (3 Points)\n\n### Standard 3.1 (Max 1 Point): Clear Naming\n- Sub-standard 3.1.1: [1 Point | Criterion] All model and field names must align with business semantics, prohibiting jargon and abbreviations (e.g., `x1`, `val`). The same concept must not use multiple names; otherwise, 0 points.\n\n### Standard 3.2 (Max 2 Points): Reasonable Layering\n- Sub-standard 3.2.1: [1 Point | Criterion] Layer boundaries must be clear: raw → staging (cleansing) → intermediate (derived metrics) → marts (aggregated exposure). If definitions cross layers or responsibilities are ambiguous, 0 points.\n- Sub-standard 3.2.2: [1 Point | Criterion] Core business definitions must be centralized in the intermediate layer, not scattered in marts or staging. If definitions are pushed down or moved up, 0 points.\n\n## Scoring Instructions\n- Each sub-standard must be scored based on **direct evidence from model documentation, data structures, or field definitions**; if no clear evidence can be found, score 0 points.\n- Scoring must not rely on subjective inference or reasonable assumptions.\n- `ALLOW_MULTI_MODEL=YES`."}
{"id": "dacomp-de-arch-019", "question": "In the past 90 days, have there been any signals of \"prolonged customer service processes\" for Enterprise plan customers (`plan_name=Enterprise`)? I need an investigation checklist aggregated by company: see if each company's percentage of SLA breach conversations has increased, broken down by channel (`email`/`messenger`/`other`); also provide the median reopen count, median time to first/last close, and median first response time for that company's conversations; then compare if these companies have lower 7-day/30-day active contact volumes and registered retention rates, and note the data continuity (are there any missing days). Finally, please attribute these problem companies to the admin teams involved in the conversations, list the Top 3 teams and their channel distribution, so we can coordinate training and shift scheduling optimization.", "rubric": "# Evaluation Criteria for Intercom Customer Service Process Lengthening and SLA Investigation Checklist\n\n## [Total Score | 30 Points] Three Fixed Requirements\n- Requirement I: Business Alignment and Semantic Accuracy (23 Points)\n- Requirement II: Technical Feasibility and Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n## Requirement I: Business Alignment and Semantic Accuracy (23 Points)\n\n### Criterion 1.1 (Max 20 Points): Functional Point Coverage\n- Path 1.1.A (20 Points): Complete Coverage\n  - Sub-criterion 1.1.A.1: [1 Point | Condition] The model output must contain `company_id` and `company_name` fields. `company_id` must be unique and non-null, and `company_name` must be a string with no missing or duplicate values; otherwise, 0 points.\n  - Sub-criterion 1.1.A.2: [1 Point | Condition] A company-level plan field `plan_name` must exist, and its value domain must be a predefined set (e.g., free/pro/enterprise); otherwise, 0 points.\n  - Sub-criterion 1.1.A.3: [1 Point | Condition] A conversation-level SLA status field `sla_status` must exist, and its enum values must be limited to [met, violated, pending]; otherwise, 0 points.\n  - Sub-criterion 1.1.A.4: [1 Point | Condition] A channel dimension field `conversation_type` must exist, and its value domain must be strictly [email, messenger, other]; otherwise, 0 points.\n  - Sub-criterion 1.1.A.5: [1 Point | Condition] A conversation creation time `conversation_created_at` must exist, and it must support filtering data for the last 90 days based on this field; otherwise, 0 points.\n  - Sub-criterion 1.1.A.6: [1 Point | Condition] The mapping relationship between conversation, contact, and company must exist explicitly (i.e., joinable via foreign keys); otherwise, 0 points.\n  - Sub-criterion 1.1.A.7: [1 Point | Condition] It must be possible to calculate the company-level percentage of SLA-violating conversations, depending on `total_conversations` and `sla_status`. Breakdown by channel must be allowed. If any required column is missing, 0 points.\n  - Sub-criterion 1.1.A.8: [1 Point | Condition] A company-level median reopens field `p50_reopens` must exist, and its data type must be numeric; otherwise, 0 points.\n  - Sub-criterion 1.1.A.9: [1 Point | Condition] A company-level median time to first response field `p50_time_to_first_response_min` must exist, and its unit must be minutes; otherwise, 0 points.\n  - Sub-criterion 1.1.A.10: [1 Point | Condition] A company-level median time to first close field `p50_time_to_first_close_min` must exist, and its unit must be minutes; otherwise, 0 points.\n  - Sub-criterion 1.1.A.11: [1 Point | Condition] A company-level median time to last close field `p50_time_to_last_close_min` must exist, and its unit must be minutes; otherwise, 0 points.\n  - Sub-criterion 1.1.A.12: [1 Point | Condition] A 7-day active contacts count field `contacts_active_7d` must exist and be non-negative; otherwise, 0 points.\n  - Sub-criterion 1.1.A.13: [1 Point | Condition] A 30-day active contacts count field `contacts_active_30d` must exist and be non-negative; otherwise, 0 points.\n  - Sub-criterion 1.1.A.14: [1 Point | Condition] A 7-day registration retention rate `registration_retention_7d` must exist, with a value range of [0,1]; otherwise, 0 points.\n  - Sub-criterion 1.1.A.15: [1 Point | Condition] A 30-day registration retention rate `registration_retention_30d` must exist, with a value range of [0,1]; otherwise, 0 points.\n  - Sub-criterion 1.1.A.16: [1 Point | Condition] A missing data days field `missing_days` must exist and be an integer; otherwise, 0 points.\n  - Sub-criterion 1.1.A.17: [1 Point | Condition] A date continuity boolean flag `is_date_continuous` must exist and only take values from [true, false]; otherwise, 0 points.\n  - Sub-criterion 1.1.A.18: [1 Point | Condition] Team key fields `team_id` and `team_name` must exist, and `team_id` must be unique; otherwise, 0 points.\n  - Sub-criterion 1.1.A.19: [1 Point | Condition] An admin attribution field must exist: at least one of `first_close_by_admin_id` or `last_close_by_admin_id`; otherwise, 0 points.\n  - Sub-criterion 1.1.A.20: [1 Point | Condition] It must be possible to derive the team-channel distribution by linking `conversation_type` with the admin→team mapping; otherwise, 0 points.\n\n- Path 1.1.B (3 Points): Basic Coverage\n  - Sub-criterion 1.1.B.1: [1 Point | Condition] Primary keys for at least three entity types—company (`company_id`), admin (`admin_id`), and team (`team_id`)—must exist; otherwise, 0 points.\n  - Sub-criterion 1.1.B.2: [1 Point | Condition] At least `conversation_created_at` and an activity/retention metric (either 7d or 30d) must exist; otherwise, 0 points.\n  - Sub-criterion 1.1.B.3: [1 Point | Condition] At the company grain, at least one median metric (`p50_time_*` or `p50_reopens`) must exist; otherwise, 0 points.\n\n### Criterion 1.2 (Max 3 Points): Metric/Feature Definitions\n  - Sub-criterion 1.2.1: [1 Point | Condition] The output must include at least an entity (company/team/admin/channel) and a time frame (e.g., last 90 days/7 days/30 days) or a statistical definition (median/percentage/count); otherwise, 0 points.\n  - Sub-criterion 1.2.2: [1 Point | Condition] The handling logic for missing/abnormal/negative values/future times must be explicitly stated in the data contract or tests; 0 points if no documentation exists.\n  - Sub-criterion 1.2.3: [1 Point | Condition] Definitions must be consistent across outputs:\n    - Median metrics are all in minutes;\n    - Channel enums are consistently [email, messenger, other];\n    - `sla_status` definition is consistent;\n    Any inconsistency results in 0 points.\n\n## Requirement II: Technical Feasibility and Structural Completeness (4 Points)\n\n### Criterion 2.1 (Max 2 Points): Format/Structure Check\n  - Sub-criterion 2.1.1: [1 Point | Condition] The model must have a `name`, declare upstream dependencies (`source_models`), and define its grain (`grain`); otherwise, 0 points.\n  - Sub-criterion 2.1.2: [1 Point | Condition] The model must list all fields and their types (`columns`); 0 points if any are missing or have an unknown type.\n\n### Criterion 2.2 (Max "}
{"id": "dacomp-de-arch-020", "question": "When reviewing our budget recently, we found that the performance of many email campaigns was being diluted by \"flow-based messages\" (e.g., cart reminders, welcome flows). Please provide me with an attribution list dimensioned by \"Campaign/Flow Variant\":\n\nFor each variant over the past 90 days, provide statistics for its attributed structured event funnel (`Impressions` / `Opens` / `Clicks` / `Orders` / `Refunds`), along with its Net GMV, conversion rates (`Open Rate`, `Click-through Rate`, `Order Rate`), and the attribution difference between the \"First Touch\" and \"Last Touch\" models.\n\nAdditionally, please deduplicate cross-session repeated touchpoints for the same user to avoid attributing multiple actions to the same variant. Combine \"Campaigns\" and \"Flows\" into a single, unified list, but ensure they are distinguishable from one another.\n\nFinally, roll up these metrics to a weekly granularity. For each week, annotate the percentage deviation from the median of that variant's past four weeks. This will allow me to see at a glance which variants are amplifying or dragging down overall revenue.", "rubric": "# Klaviyo Attribution Checklist & Variation Metrics Evaluation Criteria\n\n## [Total Score | 37 Points] Fixed Three Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (30 points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 points)\n- Requirement III: Design Quality (3 points)\n\n## Requirement I: Business Alignment & Semantic Accuracy (30 points)\n\n### Criterion 1.1 (Max 27 points): Feature Point Coverage\n- Path 1.1.A (27 points): **Complete Coverage**\n  - Sub-criterion: Unified \"Campaign/Flow\" details (differentiated by type)【1 point | Judging Condition】A touch type differentiation field exists: `session_touch_type` ∈ {'campaign','flow'}, and is linked with `last_touch_campaign_id` / `last_touch_flow_id`.\n  - Sub-criterion: Variation dimension identifier【1 point | Judging Condition】`variation_id` persists in event and aggregate models and is used as a grouping key.\n  - Sub-criterion: Impression (email impression) count【1 point | Judging Condition】`count_received_email`.\n  - Sub-criterion: Open count【1 point | Judging Condition】`count_opened_email`.\n  - Sub-criterion: Click count【1 point | Judging Condition】`count_clicked_email`.\n  - Sub-criterion: Placed order count【1 point | Judging Condition】`count_placed_order`.\n  - Sub-criterion: Refunded order count【1 point | Judging Condition】`count_refunded_order`.\n  - Sub-criterion: Ordered product count【1 point | Judging Condition】`count_ordered_product`.\n  - Sub-criterion: Net GMV (touch level)【1 point | Judging Condition】`gmv_net`.\n  - Sub-criterion: Gross GMV (touch level)【1 point | Judging Condition】`gmv_gross`.\n  - Sub-criterion: Event-level net revenue【1 point | Judging Condition】`event_net_revenue`.\n  - Sub-criterion: Open rate (opened/received)【1 point | Judging Condition】`email_open_rate`.\n  - Sub-criterion: Click-to-open rate (clicked/opened)【1 point | Judging Condition】`email_click_to_open_rate`.\n  - Sub-criterion: Placed order rate (site→order)【1 point | Judging Condition】`site_to_order_rate`.\n  - Sub-criterion: Product view to order rate【1 point | Judging Condition】`product_view_to_order_rate`.\n  - Sub-criterion: Last touch attribution identifier【1 point | Judging Condition】`last_touch_id` / `session_touch_type` exists and aligns with sub-event attribution resolution.\n  - Sub-criterion: Last touch attribution to \"Campaign/Flow\" ID【1 point | Judging Condition】`last_touch_campaign_id` / `last_touch_flow_id`.\n  - Sub-criterion: Person-touch-variation group deduplication【1 point | Judging Condition】The granularity of `klaviyo__person_campaign_flow` is `person_id` × `last_touch_campaign_id`/`flow_id` × `variation_id` × `source_relation`.\n  - Sub-criterion: Deduplicated unique people count (touch level)【1 point | Judging Condition】`total_count_unique_people` and the `unique_count_*` series exist.\n  - Sub-criterion: Daily granularity rolling details【1 point | Judging Condition】`klaviyo__events` contains the `day_*` series.\n  - Sub-criterion: Unified detail table accommodates both \"Campaign/Flow\" types【1 point | Judging Condition】The same detail model contains both `last_touch_campaign_id` and `last_touch_flow_id` fields.\n  - Sub-criterion: 90-day window【1 point | Judging Condition】A clear 90-day filter or window field/logic exists (gold layer requirement); 0 points if missing (the current data_contract does not reflect a fixed 90-day window).\n  - Sub-criterion: Weekly granularity summary (by variation)【1 point | Judging Condition】Weekly aggregated metric output exists (gold layer requirement); 0 points if missing (current metrics are daily and span-based).\n  - Sub-criterion: 4-week median deviation annotation【1 point | Judging Condition】A field for \"deviation from the median of the past four weeks\" or an equivalent column exists (gold layer requirement); 0 points if missing.\n  - Sub-criterion: First touch vs. last touch two-model comparison【1 point | Judging Condition】A \"first touch\" attribution identifier or field exists for comparison with \"last touch\" (gold layer requirement); 0 points if missing (currently only `last_touch_*` is specified).\n\n### Criterion 1.2 (Max 3 points): Metric/Feature Definition\n- Path 1.2.A (3 points): **Clear Definition**\n  - Sub-criterion: Each output item includes at least two types of elements (by whom/time range or alignment/statistical rule)【1 point | Judging Condition】For example, `klaviyo__events` provides `person_id` + `occurred_on` + metrics; touch-level models provide `last_touch_*` + `variation_id` + metrics.\n  - Sub-criterion: Default handling definition for missing/abnormal/duplicate/negative/future-time values【1 point | Judging Condition】Splitting numerical values into negative/positive and net amounts; ratio is NULL when the denominator is 0; non-negative validation for price fields.\n  - Sub-criterion: Consistent definition across output items【1 point | Judging Condition】`email_open_rate`, `email_click_to_open_rate`, `product_view_to_order_rate`, `site_to_order_rate` are defined consistently in the intermediate and marts layers.\n\n## Requirement II: Technical Feasibility & Structural Completeness (4 points)\n\n### Criterion 2.1 (Max 2 points): Format/Structure Check\n- Path 2.1.A (2 points): **Minimum Four Elements of a Model**\n  - Sub-criterion: Has upstream dependencies (`source_models`)【1 point | Judging Condition】Each layer follows dependency declarations.\n  - Sub-criterion: Has field definitions + types (`columns`)【1 point | Judging Condition】Each model's `columns` include `data_type`.\n\n### Criterion 2.2 (Max 2 points): Dependency Closure\n- Path 2.2.A (2 points): **Buildable Dependency Graph**\n  - Sub-criterion: No dangling references【1 point | Judging Condition】All referenced upstream models are defined in the contract.\n  - Sub-criterion: External sources have clear pointers and boundary descriptions【1 point | Judging Condition】`staging` only references `raw.*`, and `source_table` is specified.\n\n## Requirement III: Design Quality (3 points)\n\n### Criterion 3.1 (Max 1 point): Clear Naming\n- Sub-criterion: Consistent abbreviations/naming conventions【1 point | Judging Condition】Naming consistency for `email_open_rate` / `email_click_to_open_rate`.\n\n### Criterion 3.2 (Max 2 points): Reasonable Layering\n- Sub-criterion: Clear layer responsibilities (raw/intermediate/marts)【1 point | Judging Condition】`raw`→`staging`→`intermediate`→`marts` are clearly defined and each serves its purpose.\n- Sub-criterion: Avoid \"God tables,\" ensure module cohesion and clear dependencies【1 point | Judging Condition】Event, campaign, flow, and person tables have clear responsibilities; person-touch is a separate model.\n\n### Scoring Notes\n- Each sub-criterion is scored as 1/0; no evidence found → 0 points.\n- ALLOW_MULTI_MODEL=YES."}
{"id": "dacomp-de-arch-021", "question": "Over the past six months, our net MRR has appeared to be growing, but feedback from Operations is that \"high revenue doesn't necessarily mean the product is selling well.\" This is because for some customers, shipping and tax fees account for a high percentage of their bills, while at the same time, their subscriptions have entered the final billing cycle, posing a high renewal risk.\n\nPlease create a revenue quality profile on a \"customer × month\" basis: break down the percentages of line items, shipping, and tax; overlay subscription billing progress; and identify customer-months where \"net MRR is high, but there is ≤1 billing cycle remaining before expiration and the shipping fee percentage is ≥30%.\" Also, supplement this with new customer status and average order value as sorting dimensions to form a list that can be directly used for renewal intervention and logistics optimization.", "rubric": "# Net MRR Revenue Quality Profile Evaluation Criteria\n\n## [Total Score | 24 Points] Three Core Requirements\n- Requirement I: Business Alignment and Semantic Accuracy (17 points)\n- Requirement II: Technical Feasibility and Structural Completeness (4 points)\n- Requirement III: Design Quality (3 points)\n\n## Requirement I: Business Alignment and Semantic Accuracy (17 points)\n\n### Criterion 1.1 (Max 14 points): Feature Coverage\n- Path 1.1.A (14 points): Complete Coverage\n  - 1.1.A.1 Sub-criterion [1 point | Condition] A model at \"customer × month\" granularity exists, which must include both `customer_id` and `date_month` columns, with the grain explicitly defined as *per customer per month*; models with only daily or weekly granularity do not score.\n  - 1.1.A.2 Sub-criterion [1 point | Condition] A \"Net MRR\" monthly metric column (e.g., `calculated_net_order_mrr`) exists, which must be usable as a filtering and sorting metric, and the field must be defined in `columns`.\n  - 1.1.A.3 Sub-criterion [1 point | Condition] A unified breakdown of \"line item/shipping/tax\" by \"charge components\" exists, which must have `line_item_type` and `amount` columns for breaking down proportions.\n  - 1.1.A.4 Sub-criterion [1 point | Condition] `line_item_type` must cover at least three types of identifiers: \"charge line\", \"shipping\", and \"tax\". Missing any one of these results in 0 points.\n  - 1.1.A.5 Sub-criterion [1 point | Condition] Shipping amount and order line summary for each order are available, which must include both `total_shipping` and `order_line_item_total`, and `calculated_order_total_price` must be calculable.\n  - 1.1.A.6 Sub-criterion [1 point | Condition] A tax amount source field to support \"tax proportion\" is available, which must have an `amount` corresponding to `line_item_type='tax'`; absence results in 0 points.\n  - 1.1.A.7 Sub-criterion [1 point | Condition] A \"line item amount\" source field to support \"line item proportion\" is available, such as an `amount` corresponding to `line_item_type='charge line'`, or the order-level `order_line_item_total`.\n  - 1.1.A.8 Sub-criterion [1 point | Condition] Columns related to \"subscription charge progress\" are available, which must include the `charges_until_expiration` field.\n  - 1.1.A.9 Sub-criterion [1 point | Condition] The subscription model must contain a `customer_id` primary key field, which can be joined and projected to the \"customer × month\" view.\n  - 1.1.A.10 Sub-criterion [1 point | Condition] Subscription progress must include successful/queued charge counts and standardized charge interval in days, providing at least two of `count_successful_charges`, `count_queued_charges`, and `charge_interval_frequency_days`.\n  - 1.1.A.11 Sub-criterion [1 point | Condition] An \"is new customer\" flag field, such as `is_new_customer`, exists and must be defined in the customer details model.\n  - 1.1.A.12 Sub-criterion [1 point | Condition] An \"average order amount\" field, such as `avg_order_amount`, exists and must be defined in the `columns` of the order or customer model.\n  - 1.1.A.13 Sub-criterion [1 point | Condition] \"Customer × day\" and \"week/month/year\" derived time anchors exist, which must provide at least `date_day` and `date_month` simultaneously and ensure a mapping logic between them.\n  - 1.1.A.14 Sub-criterion [1 point | Condition] Daily and monthly aggregations for the \"realized\" order metric must be consistent, requiring that `calculated_order_total_price_realized` can be aggregated into `calculated_net_order_mrr`.\n\n- Path 1.1.B (6 points): Basic Coverage (used as fallback scoring when complete coverage is not achieved)\n  - 1.1.B.1 Sub-criterion [1 point | Condition] Entity Type Coverage: Has at least one of `customer_id` and `subscription_id`, and they can be associated via a join.\n  - 1.1.B.2 Sub-criterion [1 point | Condition] Time Type Coverage: Must have `date_month` or an equivalent monthly time anchor.\n  - 1.1.B.3 Sub-criterion [1 point | Condition] Aggregation Type Coverage: A monthly amount aggregation field, such as `calculated_net_order_mrr`, exists.\n  - 1.1.B.4 Sub-criterion [1 point | Condition] Component Coverage: Has at least a source field for `shipping` or `tax` amount.\n  - 1.1.B.5 Sub-criterion [1 point | Condition] Risk Coverage: Has at least `subscription_status` or `charges_until_expiration`.\n  - 1.1.B.6 Sub-criterion [1 point | Condition] User Attribute Coverage: Has at least `is_new_customer` or `avg_order_amount`.\n\n### Criterion 1.2 (Max 3 points): Metric/Feature Definition\n- Path 1.2.A (3 points): Clear Definitions\n  - 1.2.A.1 Sub-criterion [1 point | Condition] Each output item must include at least two of the following three types of elements: \"entity (e.g., customer_id) + time range/alignment (e.g., date_month) + statistical metric (e.g., calculated_net_order_mrr)\".\n  - 1.2.A.2 Sub-criterion [1 point | Condition] There must be default handling for missing/abnormal values (e.g., missing amounts treated as 0, refunds default to 0, negative values filtered), and this logic must be traceable in the `business_logic` or `columns` of the relevant models.\n  - 1.2.A.3 Sub-criterion [1 point | Condition] Definitions must be consistent across output items: e.g., if total order amount is defined as \"sum of line items + shipping\", the \"day→month\" aggregation logic must be strictly consistent.\n\n\n## Requirement II: Technical Feasibility and Structural Completeness (4 points)\n\n### Criterion 2.1 (Max 2 points): Format/Structure Check\n- Path 2.1.A (2 points): Model's Minimum Four Elements\n  - 2.1.A.1 Sub-criterion [1 point | Condition] A model identifier (`name`) must exist and match the business semantics; upstream dependencies (`source_models`) must exist and conform to layering (staging→intermediate→marts).\n  - 2.1.A.2 Sub-criterion [1 point | Condition] Fields and their data types (`columns.data_type`) must be defined; if type information is missing, this scores 0 points.\n\n### Criterion 2.2 (Max 2 points): Dependency Closure\n- Path 2.2.A (2 points): Buildable Dependency Graph\n  - 2.2.A.1 Sub-criterion [1 point | Condition] The dependency graph must have no dangling references; all referenced upstream model names must be resolvable.\n  - 2.2.A.2 Sub-criterion [1 point | Condition] External sources have clear boundary descriptions: staging points only to raw, intermediate points only to staging, and marts point only to intermediate.\n\n\n## Requirement III: Design Quality (3 points)\n\n### Criterion 3.1 (Max 1 point): Clear Naming\n  - 3.1.1 Sub-criterion [1 point | Condition] Names must be semantic, using domain and layer prefixes (e.g., `stg_/int_/recharge__`), with a consistent naming convention (snake_case, lowercase, double underscore for themes), and must not contain jargon or ambiguity.\n\n### Criterion 3.2 (Max 2 points): Reasonable Layering\n  - 3.2.1 Sub-criterion [1 point | Condition] Business definitions must be centralized in the corresponding model description or `business_logic`, not scattered and conflicting (e.g., definitions for realized/subscription progress must be centrally defined).\n  - 3.2.2 Sub-criterion [1 point | Condition] Avoid \"God tables\"; require modular cohesion and clear dependencies (split by function into `billing_history` / `charge_line_item_history` / `customer` / `subscription` / `mrr`, etc.), and do not allow a single model to carry too many responsibilities.\n\n\n### Scoring Notes\n- Each sub-criterion is scored as 1/0; No evidence found → 0 points.\n- ALLOW_MULTI_MODEL=YES."}
{"id": "dacomp-de-arch-022", "question": "Over the past two years, we have aimed to increase the click-through rate (CTR) on each platform by at least +15% by adjusting the monthly content type ratio, without increasing the total number of posts or sacrificing the 12-week retention of new accounts. Please provide the \"recommended content mix\" by platform and by month (e.g., `Twitter: Text 40% / Media 60%`), the corresponding maximum posting frequency, and the mix's expected CTR and engagement rate. Prioritize combinations with a higher number of active accounts for the month and more stable day-to-day performance.", "rubric": "# Social Media Content Mix Recommendation Evaluation Criteria (Strict Version)\n\n## [Total Score | 22 Points] Three Fixed Requirements\n- Requirement I: Business Alignment and Semantic Accuracy (16 points)\n- Requirement II: Technical Feasibility and Structural Completeness (4 points)\n- Requirement III: Design Quality (2 points)\n\n\n## Requirement I: Business Alignment and Semantic Accuracy (16 points)\n\n### Standard 1.1 (Max 12 points): Feature Point Coverage\n- Path 1.1.A (12 points): Complete Coverage\n  - 1.1.A.1 Sub-criterion [1 point | Condition] The `platform` dimension must exist and be consistent across all models; 0 points if there are spelling differences or it is missing.\n  - 1.1.A.2 Sub-criterion [1 point | Condition] A `month` granularity field must exist for CTR aggregation, and its grain must be explicitly declared as `per platform per month`.\n  - 1.1.A.3 Sub-criterion [1 point | Condition] A `content_type` or `content_format` dimension must exist for evaluating the mix ratio; it must be mappable via enumeration.\n  - 1.1.A.4 Sub-criterion [1 point | Condition] `clicks` must exist, be a non-negative integer, and have a consistent definition across different platforms.\n  - 1.1.A.5 Sub-criterion [1 point | Condition] `impressions` must exist, be non-negative, and have a range check in its tests.\n  - 1.1.A.6 Sub-criterion [1 point | Condition] CTR (`ctr`) must exist, be provided in both monthly and daily aggregation models, and must have a test constraining its range to 0–1.\n  - 1.1.A.7 Sub-criterion [1 point | Condition] Engagement rate (`engagement_rate` or `calculated_engagement_rate`) must exist and have a 0–1 test.\n  - 1.1.A.8 Sub-criterion [1 point | Condition] `posts_count` (post frequency) must exist, output by platform × month/day × content type.\n  - 1.1.A.9 Sub-criterion [1 point | Condition] `active_accounts_count` must exist, output by platform × month/day × content type.\n  - 1.1.A.10 Sub-criterion [1 point | Condition] Retention metrics `retention_rate`, `cohort_pages`, and `retained_pages` must exist, including `week_offset ≤ 12` for 12-week retention evaluation.\n  - 1.1.A.11 Sub-criterion [1 point | Condition] Stability/quality proxy metrics (e.g., `daily_platform_rank` / `platform_lifetime_rank` / `content_quality_score`) must exist and be usable for prioritizing \"more stable daily performance.\"\n  - 1.1.A.12 Sub-criterion [1 point | Condition] When outputting the recommended mix, the three dimensions of platform, month, and content type (`platform`, `month`, `content_type`) must all be present; 0 points if any dimension is missing.\n\n### Standard 1.2 (Max 4 points): Metric/Feature Definition\n- Path 1.2.A (4 points): Clear Definition\n  - 1.2.A.1 Sub-criterion [1 point | Condition] Each output item must contain at least two of the following element types: entity (e.g., `platform`/`content_type`), time range (e.g., `month`/`day`), and statistical definition (e.g., `ctr`, `engagement_rate`).\n  - 1.2.A.2 Sub-criterion [1 point | Condition] There must be default handling for missing, abnormal, duplicate, negative, or future time values, with boundaries explicitly stated in tests (e.g., non-null, range 0–1, non-negative).\n  - 1.2.A.3 Sub-criterion [1 point | Condition] Definitions must be consistent across outputs: clicks/impressions/engagement rate must be defined consistently across daily, monthly, and platform dimensions and must not be contradictory.\n  - 1.2.A.4 Sub-criterion [1 point | Condition] Content type/format must be mappable across different platforms and usable for mix evaluation; 0 points if it is defined for only some platforms.\n\n\n## Requirement II: Technical Feasibility and Structural Completeness (4 points)\n\n### Standard 2.1 (Max 2 points): Format/Structure Check\n- Path 2.1.A (2 points): Minimum Model Components\n  - 2.1.A.1 Sub-criterion [1 point | Condition] Each model must have a unique identifier `name`, which cannot be missing or duplicated.\n  - 2.1.A.2 Sub-criterion [1 point | Condition] Upstream dependencies `source_models` must be declared, otherwise 0 points.\n\n### Standard 2.2 (Max 2 points): Dependency Closure\n- Path 2.2.A (2 points): Buildable Dependency Graph\n  - 2.2.A.1 Sub-criterion [1 point | Condition] The dependency graph must have no cycles or self-references, following the `staging` → `intermediate` → `marts` pattern; 0 points if a self-cycle exists.\n  - 2.2.A.2 Sub-criterion [1 point | Condition] External source boundaries must be clear: `staging` must only point to `raw`, `intermediate` only to `staging`, and `marts` only to `intermediate`.\n\n## Requirement III: Design Quality (2 points)\n\n### Standard 3.1 (Max 1 point): Clear Naming\n- 3.1.1 Sub-criterion [1 point | Condition] Names must be semantic, follow a unified convention, and avoid jargon or ambiguity (e.g., `platform`, `page_id`, `post_id`, `created_timestamp`, `ctr`, `engagement_rate`). Inconsistent abbreviations are not allowed.\n\n### Standard 3.2 (Max 1 point): Reasonable Layering\n- 3.2.1 Sub-criterion [1 point | Condition] Layer responsibilities must be clear: the raw, intermediate, and mart layers must each have their own function. Business logic should be concentrated in the intermediate or mart layers, avoiding \"God tables.\" Dependencies must be unidirectional (`staging`→`intermediate`→`marts`).\n\n### Scoring Notes\n- Each sub-criterion is scored as 1 or 0; no evidence found → 0 points.\n- ALLOW_MULTI_MODEL=YES."}
{"id": "dacomp-de-arch-023", "question": "In our recent ad placements, the same ad creative was linked to different landing page domains (and different UTM campaign identifiers). I want to figure out something very specific: in the past two months, which `domain × UTM campaign` combinations were able to consistently achieve a higher return on ad spend (ROAS) across multiple campaigns without sacrificing click-through rate (CTR)? Please look at the trends on a weekly basis. Don't just look at the top ROAS; you need to exclude cases with occasional high values and insufficient data volume. Provide a list of combinations that have performed consistently in at least three different campaigns, along with their fluctuation range.", "rubric": "# TikTok Ads \"Domain × UTM campaign\" Weekly ROAS and CTR Stability Analysis Evaluation Criteria (Strict Version)\n\n## [Total Score | 28 Points] Three Fixed Requirements\n- Requirement I: Business Alignment and Semantic Accuracy (21 Points)\n- Requirement II: Technical Feasibility and Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n## Requirement I: Business Alignment and Semantic Accuracy (21 Points)\n\n### Criterion 1.1 (Max 18 Points): Functional Point Coverage\n- Path 1.1.A (18 Points): Complete Coverage (Item-by-item check against business objects, metrics, dimensions, and granularity reflected in gold.yaml)\n  - 1.1.A.1 Sub-criterion [1 Point | Condition] The domain dimension `url_host` exists and must be provided and non-null in the URL report model.\n  - 1.1.A.2 Sub-criterion [1 Point | Condition] The UTM campaign dimension `utm_campaign` exists and must be provided and non-null in the URL report model.\n  - 1.1.A.3 Sub-criterion [1 Point | Condition] `base_url` exists with a not_null test; absence results in 0 points.\n  - 1.1.A.4 Sub-criterion [1 Point | Condition] Campaign dimensions are complete; both `campaign_id` and `campaign_name` must exist.\n  - 1.1.A.5 Sub-criterion [1 Point | Condition] Ad group dimensions are complete; both `ad_group_id` and `ad_group_name` must exist.\n  - 1.1.A.6 Sub-criterion [1 Point | Condition] Ad dimensions are complete; both `ad_id` and `ad_name` must exist.\n  - 1.1.A.7 Sub-criterion [1 Point | Condition] The date granularity `date_day` exists with a not_null test and can be used for weekly aggregation.\n  - 1.1.A.8 Sub-criterion [1 Point | Condition] The CTR output metric `daily_ctr` exists with a BETWEEN 0 AND 100 test, and must be uniformly defined as a percentage.\n  - 1.1.A.9 Sub-criterion [1 Point | Condition] Fields required for ROAS calculation are complete; both `spend` and `total_conversion_value` (or alternative fields like total_purchase_value/total_sales_lead_value) must exist.\n  - 1.1.A.10 Sub-criterion [1 Point | Condition] Click and impression fields are complete; both `clicks` and `impressions` must exist.\n  - 1.1.A.11 Sub-criterion [1 Point | Condition] Sample size fields have non-negative tests: `impressions >= 0` and `clicks >= 0`.\n  - 1.1.A.12 Sub-criterion [1 Point | Condition] Spend field has a non-negative test: `spend >= 0`.\n  - 1.1.A.13 Sub-criterion [1 Point | Condition] Conversion value field has a non-negative test: `total_conversion_value >= 0`.\n  - 1.1.A.14 Sub-criterion [1 Point | Condition] A constraint must exist for the landing page URL; the URL model must only contain non-null records for `landing_page_url`.\n  - 1.1.A.15 Sub-criterion [1 Point | Condition] A creative identifier field exists, which can be used to determine \"same creative\" (e.g., `creative_material_mode` or `ad_name`).\n  - 1.1.A.16 Sub-criterion [1 Point | Condition] The model level is URL detail; it must include `url_path` and `utm_*` fields.\n  - 1.1.A.17 Sub-criterion [1 Point | Condition] The model granularity must include `campaign_id` to ensure stability can be evaluated across campaigns.\n  - 1.1.A.18 Sub-criterion [1 Point | Condition] Must support observing trends by week; daily metrics must be complete and aggregatable to a weekly level.\n\n### Criterion 1.2 (Max 3 Points): Metric/Feature Definition\n- Path 1.2.A (3 Points): Clear Definition\n  - 1.2.A.1 Sub-criterion [1 Point | Condition] Each output item must include both primary dimensions (e.g., campaign/ad_group/ad) and a time dimension (date_day).\n  - 1.2.A.2 Sub-criterion [1 Point | Condition] Handling of missing/abnormal values must be explicit (e.g., default to 0, type-safe) and documented or tested in the intermediate layer.\n  - 1.2.A.3 Sub-criterion [1 Point | Condition] Metric definitions must be consistent across models (e.g., `daily_ctr`, CPC, CPM have consistent definition boundaries), with boundary tests.\n\n\n## Requirement II: Technical Feasibility and Structural Completeness (4 Points)\n\n### Criterion 2.1 (Max 2 Points): Format/Structure Check\n- Path 2.1.A (2 Points): Minimum Four Elements of a Model\n  - 2.1.A.1 Sub-criterion [1 Point | Condition] Has upstream dependencies `source_models`, and references must follow hierarchical rules.\n  - 2.1.A.2 Sub-criterion [1 Point | Condition] Has field definitions and types; `columns` must declare `data_type`.\n\n### Criterion 2.2 (Max 2 Points): Dependency Closure\n- Path 2.2.A (2 Points): Buildable Dependency Graph\n  - 2.2.A.1 Sub-criterion [1 Point | Condition] Dependencies have no cycles/self-references and strictly follow rules against cross-layer/same-layer dependencies.\n  - 2.2.A.2 Sub-criterion [1 Point | Condition] External sources must have boundary descriptions; staging only references raw, intermediate only references staging, and marts only reference intermediate.\n\n\n## Requirement III: Design Quality (3 Points)\n\n### Criterion 3.1 (Max 1 Point): Clear Naming\n- 3.1.1 Sub-criterion [1 Point | Condition] The same concept does not mix multiple definitions; e.g., CTR must be uniformly defined as a percentage and have boundary tests.\n\n### Criterion 3.2 (Max 2 Points): Reasonable Layering\n- 3.2.1 Sub-criterion [1 Point | Condition] Layer responsibilities are clear (raw/intermediate/mart) and explicitly defined in `modeling_spec`.\n- 3.2.2 Sub-criterion [1 Point | Condition] Avoid \"God tables\"; models should remain cohesive with clear, unidirectional dependencies (no cross-layer/cross-row references).\n\n\n### Scoring Notes\n- Each sub-criterion is scored as 1/0; no evidence found → 0 points.\n- ALLOW_MULTI_MODEL=YES."}
{"id": "dacomp-de-arch-024", "question": "We have recently been promoting A2P compliance in the US market, but I would like to see a more practical reconciliation view: broken down by account and by month, splitting all outbound messages into two categories—\"sent using a registered Messaging Service\" and \"sent without a registered Messaging Service\"—to compare three things:\n\n1) Delivery performance: `delivered rate` and the percentage of `failed/undelivered` messages.\n2) Volume: outbound message volume and the average number of segments.\n3) Cost efficiency: total monthly SMS cost (by `price_unit`) and the average cost per successfully delivered message.\n\nIt is especially important to be able to see the trend changes for the same account before and after registration, as well as across months. This will allow us to evaluate whether A2P compliance has actually improved delivery and reduced retries or wasted costs resulting from failures.", "rubric": "# Twilio A2P Compliance Reconciliation View Evaluation Criteria\n\n## [Total Score | 29 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (22 Points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n## Requirement I: Business Alignment & Semantic Accuracy (22 Points)\n\n### Criterion 1.1 (Max 18 Points): Functional Point Coverage\n- Path 1.1.A (18 Points): Complete Coverage\n  - 1.1.A.1 Sub-criterion [1 Point | Judging Condition] `account_id` exists, allowing aggregation by account. Must be present in the output model and `not_null` (e.g., `twilio__account_overview.account_id`).\n  - 1.1.A.2 Sub-criterion [1 Point | Judging Condition] `date_month` exists, allowing alignment by month. Must be in `YYYY-MM` format and have a `not_null` test.\n  - 1.1.A.3 Sub-criterion [1 Point | Judging Condition] `direction` exists, allowing filtering for outbound. The field's value range must be a finite set (`outbound`/`inbound`).\n  - 1.1.A.4 Sub-criterion [1 Point | Judging Condition] `messaging_service_id` exists, allowing identification of whether a Messaging Service is used. The field must be joinable.\n  - 1.1.A.5 Sub-criterion [1 Point | Judging Condition] `us_app_to_person_registered` exists, clearly distinguishing between \"registered/unregistered\". It must not be missing.\n  - 1.1.A.6 Sub-criterion [1 Point | Judging Condition] A `delivered` metric or count exists, with a consistent source (e.g., `status=delivered` or `total_delivered_messages`).\n  - 1.1.A.7 Sub-criterion [1 Point | Judging Condition] A `failed` metric or count exists, with a consistent definition, and must not be confused with `undelivered`.\n  - 1.1.A.8 Sub-criterion [1 Point | Judging Condition] An `undelivered` metric or count exists, with a clear definition, independent of `failed`.\n  - 1.1.A.9 Sub-criterion [1 Point | Judging Condition] A `sent` metric or count exists, used as the denominator for delivery rate. Its definition must be consistent with `total_sent_messages`.\n  - 1.1.A.10 Sub-criterion [1 Point | Judging Condition] An outbound message volume metric exists, which can be provided by `direction=outbound` or `total_outbound_messages`.\n  - 1.1.A.11 Sub-criterion [1 Point | Judging Condition] `num_segments` or an average segment metric exists, which must be guaranteed to be ≥1 and an integer.\n  - 1.1.A.12 Sub-criterion [1 Point | Judging Condition] `price` (message price) exists. The field must be non-null and its value must be ≥0.\n  - 1.1.A.13 Sub-criterion [1 Point | Judging Condition] `price_unit` (price currency/unit) exists and is consistently preserved across all model layers.\n  - 1.1.A.14 Sub-criterion [1 Point | Judging Condition] A metric for total monthly message spend exists, which can be obtained by direct aggregation and is consistent with `price`/`price_unit`.\n  - 1.1.A.15 Sub-criterion [1 Point | Judging Condition] Supports calculation of \"average cost per successfully delivered message\". Must have both a `delivered` count and a price field.\n  - 1.1.A.16 Sub-criterion [1 Point | Judging Condition] Supports breakdown by \"registered/unregistered Messaging Service\". Must include `account_id`, `date_month`, `us_app_to_person_registered`, and be able to filter for `outbound`.\n  - 1.1.A.17 Sub-criterion [1 Point | Judging Condition] Supports cross-month trend analysis. Must ensure the `account_id` + `date_month` granularity is continuous and comparable.\n  - 1.1.A.18 Sub-criterion [1 Point | Judging Condition] Account context information can be joined and displayed. `account_name`, `account_status`, and `account_type` must exist.\n\n- Path 1.1.B (3 Points): Basic Coverage (Choose one of two scoring paths; not cumulative with 1.1.A)\n  - 1.1.B.1 Sub-criterion [1 Point | Judging Condition] Entity-related elements must cover at least one of account or messaging service (e.g., `account_id`, `messaging_service_id`).\n  - 1.1.B.2 Sub-criterion [1 Point | Judging Condition] Time-related elements must include month granularity (`date_month`).\n  - 1.1.B.3 Sub-criterion [1 Point | Judging Condition] Aggregation-related elements must cover at least one status count or cost (e.g., `delivered`/`failed`/`undelivered`/`sent`/`total_messages_spend`/`price`).\n\n### Criterion 1.2 (Max 4 Points): Metric/Feature Definitions\n- Path 1.2.A (4 Points): Clear Definitions\n  - 1.2.A.1 Sub-criterion [1 Point | Judging Condition] Each output item must contain at least two types of elements (e.g., dimension + time or dimension + statistical rule), for example, `account_id` + `date_month` + `total_sent_messages`.\n  - 1.2.A.2 Sub-criterion [1 Point | Judging Condition] Has a default handling rule for missing/non-existent statuses, explicitly \"set to 0 if it does not exist upstream\".\n  - 1.2.A.3 Sub-criterion [1 Point | Judging Condition] Currency definitions are consistent across outputs; `price_unit` must be the same at the message level and the summary level.\n  - 1.2.A.4 Sub-criterion [1 Point | Judging Condition] Denominators for delivery/failure rates have consistent definitions; `sent`/`total_messages` definitions must be globally unified.\n\n## Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n\n### Criterion 2.1 (Max 2 Points): Format/Structure Check\n- Path 2.1.A (2 Points): Model's Minimum Four Elements\n  - 2.1.A.1 Sub-criterion [1 Point | Judging Condition] The model must have a unique identifier (`name`).\n  - 2.1.A.2 Sub-criterion [1 Point | Judging Condition] The model must define fields and their types (`columns` including `data_type`).\n\n### Criterion 2.2 (Max 2 Points): Dependency Closure\n- Path 2.2.A (2 Points): Buildable Dependency Graph\n  - 2.2.A.1 Sub-criterion [1 Point | Judging Condition] No dangling references; all referenced models/columns can be resolved in the `contract`.\n  - 2.2.A.2 Sub-criterion [1 Point | Judging Condition] External sources have clear boundary definitions: `staging` only points to `raw`, `intermediate` only points to `staging`, and `marts` only points to `intermediate`.\n\n## Requirement III: Design Quality (3 Points)\n\n### Criterion 3.1 (Max 1 Point): Clear Naming\n- 3.1.1 Sub-criterion [1 Point | Judging Condition] Model and field names are semantic, avoiding jargon and ambiguity, and adopt the `twilio__` prefix + snake_case naming convention.\n\n### Criterion 3.2 (Max 2 Points): Reasonable Layering\n- 3.2.1 Sub-criterion [1 Point | Judging Condition] Business definitions are centralized; metrics are uniformly defined in"}
{"id": "dacomp-de-arch-025", "question": "In the past two months, which Twilio accounts have seen their combined cost per delivered message (message fees + usage fees, in USD) increase by more than 15%, while their traffic structure (the proportion of traffic across different messaging services) has remained stable? Please provide a list of the affected accounts, with support for drilling down to the `messaging_service` and `phone_number` levels. For each, identify whether the cost increase was caused by a drop in deliverability, a change in the service rate structure, or a rise in usage fees. Additionally, provide targeted recommendations for route/number optimization, including which services/numbers should be consolidated or migrated and the expected savings.", "rubric": "# Evaluation Criteria for \"Rising Comprehensive Cost of Delivered Messages\" With a Stable Two-Month Proportion for Twilio\n\n## [Total Score | 35 Points] Three Fixed Requirements\n- Requirement I: Business Alignment and Semantic Accuracy (28 Points)\n- Requirement II: Technical Feasibility and Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n\n## Requirement I: Business Alignment and Semantic Accuracy (28 Points)\n\n### Standard 1.1 (Max 25 Points): Feature Point Coverage\n- Path 1.1.A (25 Points): Complete Coverage\n  - 1.1.A.1 Sub-standard [1 Point | Judging Criteria] Presence of `twilio__account_overview.account_id` (`not_null`), primary key is not null and unique.\n  - 1.1.A.2 Sub-standard [1 Point | Judging Criteria] Presence of any of the columns `account_name`/`account_status`/`account_type`, allowing account attribute information to be displayed.\n  - 1.1.A.3 Sub-standard [1 Point | Judging Criteria] Presence of `date_month` (`not_null`), in `YYYY-MM` format, ensuring monthly alignment.\n  - 1.1.A.4 Sub-standard [1 Point | Judging Criteria] Presence of `date_week` (`not_null`), with a range of 1–53.\n  - 1.1.A.5 Sub-standard [1 Point | Judging Criteria] Presence of `date_day` (`not_null`), in `YYYY-MM-DD` format.\n  - 1.1.A.6 Sub-standard [1 Point | Judging Criteria] Presence of `price_unit` (`not_null`), consistent across models.\n  - 1.1.A.7 Sub-standard [1 Point | Judging Criteria] Presence of `total_outbound_messages`, with a ≥0 test.\n  - 1.1.A.8 Sub-standard [1 Point | Judging Criteria] Presence of `total_inbound_messages`, with a ≥0 test.\n  - 1.1.A.9 Sub-standard [1 Point | Judging Criteria] Presence of `total_sent_messages`, `not_null`, supports use as the denominator for delivery rate.\n  - 1.1.A.10 Sub-standard [1 Point | Judging Criteria] Presence of `total_delivered_messages`, `not_null`, supports use as the numerator for delivery rate.\n  - 1.1.A.11 Sub-standard [1 Point | Judging Criteria] Presence of either the `total_failed_messages` or `total_undelivered_messages` column, with the field being non-negative.\n  - 1.1.A.12 Sub-standard [1 Point | Judging Criteria] Presence of `total_messages` (with a ≥0 test), consistent with `sent`+`delivered`+`failed`.\n  - 1.1.A.13 Sub-standard [1 Point | Judging Criteria] Presence of `total_messages_spend` (message spend), with the unit consistent with `price_unit`.\n  - 1.1.A.14 Sub-standard [1 Point | Judging Criteria] Presence of `total_account_spend` (usage spend), distinguishable from `total_messages_spend`.\n  - 1.1.A.15 Sub-standard [1 Point | Judging Criteria] Presence of `messaging_service_id`, supporting drill-down by service dimension.\n  - 1.1.A.16 Sub-standard [1 Point | Judging Criteria] Presence of `friendly_name`, providing a human-readable name for the service.\n  - 1.1.A.17 Sub-standard [1 Point | Judging Criteria] Presence of `message_enhanced.date_month`, used for service/time combinations.\n  - 1.1.A.18 Sub-standard [1 Point | Judging Criteria] Presence of `status` (`delivered`/`failed`/`undelivered`/`sent`), definition must be globally consistent.\n  - 1.1.A.19 Sub-standard [1 Point | Judging Criteria] Presence of `price` and `price_unit` (message unit price and currency), both are not null and `price` ≥ 0.\n  - 1.1.A.20 Sub-standard [1 Point | Judging Criteria] Presence of either the `num_segments` or `num_media` column, which is ≥1 and an integer.\n  - 1.1.A.21 Sub-standard [1 Point | Judging Criteria] Presence of `int_twilio__messages.phone_number`, traceable after direction is unified.\n  - 1.1.A.22 Sub-standard [1 Point | Judging Criteria] Presence of `number_overview.phone_number` (`not_null`).\n  - 1.1.A.23 Sub-standard [1 Point | Judging Criteria] Presence of `number_overview.total_messages`, with a ≥0 test.\n  - 1.1.A.24 Sub-standard [1 Point | Judging Criteria] Presence of `number_overview.total_spend`, which is ≥0 and its unit is consistent with `total_messages_spend`.\n  - 1.1.A.25 Sub-standard [1 Point | Judging Criteria] Presence of `int_twilio__usage_record_daily.start_date` and `daily_price`, ensuring daily usage spend aligns with monthly totals.\n\n\n\n### Standard 1.2 (Max 3 Points): Metric/Feature Definitions\n- Path 1.2.A (3 Points): Clear Definitions\n  - 1.2.A.1 Sub-standard [1 Point | Judging Criteria] Each output item contains at least two types of elements: \"by whom (entity) + time granularity,\" e.g., `account_id` + `date_month`.\n  - 1.2.A.2 Sub-standard [1 Point | Judging Criteria] There is default handling or tests (e.g., `not_null`, ≥0, uniqueness) for missing, abnormal, duplicate, or negative values, and future dates.\n  - 1.2.A.3 Sub-standard [1 Point | Judging Criteria] Definitions are consistent across outputs: `price` and `price_unit` definitions are not contradictory, `status` enum sets are consistent, and monthly and daily aggregations can be aligned.\n\n\n\n## Requirement II: Technical Feasibility and Structural Completeness (4 Points)\n\n### Standard 2.1 (Max 2 Points): Format/Structure Check\n- Path 2.1.A (2 Points): Minimal Model Elements\n  - 2.1.A.1 Sub-standard [1 Point | Judging Criteria] Presence of upstream dependencies (`source_models`), which must be listed.\n  - 2.1.A.2 Sub-standard [1 Point | Judging Criteria] Presence of field definitions and types (`columns` including `data_type`).\n\n### Standard 2.2 (Max 2 Points): Dependency Closure\n- Path 2.2.A (2 Points): Buildable Dependency Graph\n  - 2.2.A.1 Sub-standard [1 Point | Judging Criteria] No circular/self-references; dependencies must be unidirectional (`staging`→`intermediate`→`marts`).\n  - 2.2.A.2 Sub-standard [1 Point | Judging Criteria] External sources have clear boundary definitions (`raw`→`staging`); cross-layer mixing is not allowed.\n\n\n\n## Requirement III: Design Quality (3 Points)\n\n### Standard 3.1 (Max 1 Point): Clear Naming\n- 3.1.1 Sub-standard [1 Point | Judging Criteria] Abbreviations and naming conventions are consistent (e.g., `date_day`/`date_week`/`date_month`; `price_unit` remains consistent).\n\n### Standard 3.2 (Max 2 Points): Reasonable Layering\n- 3.2.1 Sub-standard [1 Point | Judging Criteria] Layer responsibilities are clear: `staging` (standardization), `intermediate` (business transformation), `marts` (analytical views).\n- 3.2.2 Sub-standard [1 Point | Judging Criteria] Avoid \"God tables\"; models have a single responsibility, dependency chains are clear, and there are no cross-layer joins.\n\n\n\n### Scoring Instructions\n- Each sub-standard is scored as 1 or 0; no evidence → 0 points.\n- `ALLOW_MULTI_MODEL=YES`."}
{"id": "dacomp-de-arch-026", "question": "Over the past two months, our Twitter ad click-through rates have looked decent, but the order revenue (which we represent with `total_conversions_sale_amount`) has been lagging. Please analyze the performance by the \"Creative × Landing Page/UTM\" dimension, breaking it down by different placements, to identify the following type of issue: certain promoted tweets have high clicks and high spend at the account/campaign level, but the initial landing page they lead to (or a specific `utm_campaign`/`utm_medium` combination) has a conversion rate and ROAS significantly lower than the average for the same account during the same period. Further investigate by date to see if these issues are concentrated in specific time periods or countries/regions. Finally, provide an actionable list that includes the relevant `promoted_tweet_id`, `tweet_text`, `base_url`/`utm` combinations, and the corresponding insights (e.g., normal CPC/CTR but significantly low CVR/ROAS), so that the marketing team can prioritize investigating the landing pages and creatives.", "rubric": "# Twitter Ads \"Creative × Landing Page/UTM × Placement × Geo\" Evaluation Standard (Strict Version)\n\n## [Total Score | 27 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (20 Points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n## Requirement I: Business Alignment & Semantic Accuracy (20 Points)\n\n### Standard 1.1 (Max 17 Points): Feature Point Coverage\n- Path 1.1.A (17 Points): Complete Coverage\n  - 1.1.A.1 Sub-standard [1 Point | Judging Condition]: An account report model \"by Day × Account × Placement\" exists, including `clicks`, `impressions`, `spend`, `url_clicks`, `total_conversions`, `total_conversions_sale_amount`; `grain = date_day, account_id, placement`, and all fields are `not_null` and ≥0.\n  - 1.1.A.2 Sub-standard [1 Point | Judging Condition]: A campaign report model \"by Day × Account × Campaign × Placement\" exists; `grain = date_day, account_id, campaign_id, placement`.\n  - 1.1.A.3 Sub-standard [1 Point | Judging Condition]: A country report model \"by Day × Account × Campaign × Country × Placement\" exists; includes `country`, with the value being a two-digit ISO code.\n  - 1.1.A.4 Sub-standard [1 Point | Judging Condition]: A region report model \"by Day × Account × Campaign × Country × Region × Placement\" exists; includes `country`, `region`, and `region` is not empty.\n  - 1.1.A.5 Sub-standard [1 Point | Judging Condition]: A creative report model \"by Day × Account × Promoted Tweet × Placement\" (`marts.twitter_ads__promoted_tweet_report`) exists, with a complete granularity declaration.\n  - 1.1.A.6 Sub-standard [1 Point | Judging Condition]: A URL report model (`marts.twitter_ads__url_report`) exists, including `base_url`, `url_host`, `url_path`, `utm_source`, `utm_medium`, `utm_campaign`, `utm_content`, `utm_term`; fields are not empty.\n  - 1.1.A.7 Sub-standard [1 Point | Judging Condition]: Tweet text is available for output (`tweet_full_text`/`tweet_name`) and has a primary key relationship with `promoted_tweet_id`.\n  - 1.1.A.8 Sub-standard [1 Point | Judging Condition]: The `placement` dimension exists in all relevant reports and is part of the `grain`; enum values are complete.\n  - 1.1.A.9 Sub-standard [1 Point | Judging Condition]: Supports the breakdown of \"Creative × First Landing Page (`base_url`)/UTM Combination\" (depends on `int_twitter_ads__tweet_url_first` and `url_report`), and the combined column is `not_null`.\n  - 1.1.A.10 Sub-standard [1 Point | Judging Condition]: ROAS can be calculated (defined by `total_conversions_sale_amount` and `spend`, both ≥0).\n  - 1.1.A.11 Sub-standard [1 Point | Judging Condition]: CTR, CPC, CVR can be calculated (with non-negative constraints on `clicks`, `impressions`, `spend`, `url_clicks`, `total_conversions`, and `CTR=clicks/impressions` BETWEEN 0 AND 1).\n  - 1.1.A.12 Sub-standard [1 Point | Judging Condition]: Supports comparison within the same account over the same period (at `account_id` + `date_day` granularity), not limited to a single account.\n  - 1.1.A.13 Sub-standard [1 Point | Judging Condition]: Supports filtering at the campaign level (campaign report includes `clicks`, `spend`, `total_conversions`, `ROAS`, and can be compared with account-level aggregations).\n  - 1.1.A.14 Sub-standard [1 Point | Judging Condition]: Supports observing concentration by date (all reports include `date_day`, which is `not_null`).\n  - 1.1.A.15 Sub-standard [1 Point | Judging Condition]: Supports observing concentration by country/region (`campaign_country` and `campaign_region` reports include `country`, `region`, which are not empty).\n  - 1.1.A.16 Sub-standard [1 Point | Judging Condition]: Can output an \"actionable list\": `promoted_tweet_id`, `tweet_text`, `base_url`/UTM combination, `placement`, and key metric fields; all fields are `not_null`.\n  - 1.1.A.17 Sub-standard [1 Point | Judging Condition]: Upstream dimension tables (`accounts`/`campaigns`/`line_items`/`promoted_tweets`/`tweets`) exist, with a clear `grain` and complete foreign keys.\n\n### Standard 1.2 (Max 3 Points): Metric/Feature Definitions\n- Path 1.2.A (3 Points): Clear Definitions\n  - 1.2.A.1 Sub-standard [1 Point | Judging Condition]: Each output includes at least two types of elements: \"by whom (entity identifier) + time (day) or placement alignment + aggregation definition\", e.g., `date_day` + `account_id`/`campaign_id`/`promoted_tweet_id` + `placement`.\n  - 1.2.A.2 Sub-standard [1 Point | Judging Condition]: All metrics have non-negativity tests (`clicks`/`impressions`/`spend`/`url_clicks`/`total_conversions`/`total_conversions_sale_amount` ≥0); key primary keys (`account_id`/`campaign_id`/`promoted_tweet_id`/`base_url`) are all `not_null`.\n  - 1.2.A.3 Sub-standard [1 Point | Judging Condition]: Definitions are consistent across models: units for `spend` and `spend_micro` are consistent; definitions for `total_conversions` and `total_conversions_sale_amount` are consistent; boundaries for CTR/ROAS are clear.\n\n## Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n\n### Standard 2.1 (Max 2 Points): Format/Structure Check\n- Path 2.1.A (2 Points): Model's Minimum Four Elements\n  - 2.1.A.1 Sub-standard [1 Point | Judging Condition]: Each model has upstream dependencies (`source_models`), referencing `staging` or `intermediate`.\n  - 2.1.A.2 Sub-standard [1 Point | Judging Condition]: Each model defines fields and types (`columns` including `data_type`) and declares `tests` (`not_null`, non-negative, range validation).\n\n### Standard 2.2 (Max 2 Points): Dependency Closure\n- Path 2.2.A (2 Points): Can Build Dependency Graph\n  - 2.2.A.1 Sub-standard [1 Point | Judging Condition]: No dangling references (`source_models` must exist in the same project).\n  - 2.2.A.2 Sub-standard [1 Point | Judging Condition]: Follows layered dependencies (`staging`→`intermediate`→`marts` is unidirectional), with no cross-layer or same-layer cycles.\n\n## Requirement III: Design Quality (3 Points)\n\n### Standard 3.1 (Max 1 Point): Clear Naming\n- 3.1.1 Sub-standard [1 Point | Judging Condition]: Metric and dimension names are consistent with business meaning; the same concept does not use multiple definitions (e.g., units for `spend`/`spend_micro` are clear, CTR is always `clicks`/`impressions` × 100%).\n\n### Standard 3.2 (Max 2 Points): Reasonable Layering\n- 3.2.1 Sub-standard [1 Point | Judging Condition]: Layer responsibilities are clear (`staging` for cleaning; `intermediate` for business definitions/dimension tables; `marts` for analytical report output).\n- 3.2.2 Sub-standard [1 Point | Judging Condition]: Avoids \"God tables\"; models are cohesive, and the dependency chain is clear; URL-related definitions are centralized in `int_twitter_ads__tweet_url_first` and `twitter_ads__url_report`.\n\n### Scoring Notes\n- Each sub-standard is scored as 1/0; no evidence found → 0 points.\n- `ALLOW_MULTI_MODEL=YES`."}
{"id": "dacomp-de-arch-027", "question": "At the `project` level, Q3 net profit declined significantly compared to Q2. Please provide a monthly trend of net amounts by project, and for each project that declined by more than 15%, break it down to the `top 10 invoice line items contributing to the decline`: the contribution percentage of these line items within their respective invoices, the corresponding account type, and the associated department/region tags. Also, determine if expenses are concentrated in a few accounts (e.g., marketing, travel, etc.), whether there are changes in discounts or tax rates causing gross margin erosion, and finally, provide a list of the most likely drivers for the decline for each project.", "rubric": "# Project Monthly Net Amount Trend and Decline Driver Analysis Evaluation Criteria\n\n## [Total Score | 23 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (16 Points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n\n\n## Requirement I: Business Alignment & Semantic Accuracy (16 Points)\n\n### Criterion 1.1 (Max 13 Points): Functional Point Coverage\n- Path 1.1.A (13 Points): Complete Coverage  \n  - 1.1.A.1 Project dimension (monthly net amount trend) [1 Point | The `grain` of `xero__profit_and_loss_report` includes `report_month` + `project`, `net_amount` exists, `not_null`]\n  - 1.1.A.2 Fiscal period alignment [1 Point | `fiscal_year`, `fiscal_month`, `period_type` exist, `not_null`]\n  - 1.1.A.3 Revenue/expenses and net amount [1 Point | `revenue`, `expenses`, `net_amount`, `cumulative_net` exist, amount ≥0]\n  - 1.1.A.4 Invoice line contribution ratio [1 Point | `line_share_gross`, `line_share_net`, `invoice_gross_nonneg`, `invoice_net_nonneg`, `invoice_line_count` exist, non-negative tests]\n  - 1.1.A.5 Line sequence and cumulative amount within invoice [1 Point | `line_seq_in_invoice`, `invoice_cum_gross_nonneg` exist, non-negative tests]\n  - 1.1.A.6 Account (chart of accounts) classification [1 Point | `account_type`, `account_class`, `account_code`/`name` exist, `not_null`]\n  - 1.1.A.7 Department/region/project tags [1 Point | `region`, `department`, `project` exist and appear in both `int_` and `marts` layers]\n  - 1.1.A.8 Discount-related fields [1 Point | `discount_rate` exists (in invoice line and enriched models), range 0–1]\n  - 1.1.A.9 Tax-related fields and effective tax rate [1 Point | `tax_amount`, `tax_type`, and `effective_tax_rate` exist, amount ≥0]\n  - 1.1.A.10 Gross profit erosion identification [1 Point | `int_xero__general_ledger_fact` includes `gross_amount`, `net_amount`, `tax_amount`, amount ≥0]\n  - 1.1.A.11 Decline detection [1 Point | P&L includes `net_amount` and `report_month`/`fiscal_*`, supports month-over-month/year-over-year comparison (15% decline threshold is calculable)]\n  - 1.1.A.12 Associated contact/supplier context [1 Point | `contact_id`, `contact_name` exist, `not_null`]\n  - 1.1.A.13 Driver list [1 Point | `int_xero__tracking_categories_with_options` includes `tracking_category_name`, `option`, `not_null`]\n\n### Criterion 1.2 (Max 3 Points): Metric/Feature Definition\n- Path 1.2.A (3 Points): Clear Definition  \n\n  - 1.2.A.1 Output items are complete [1 Point | Must include at least three types of elements: \"by what (`project`/`region`/`department`/`account`, etc.)\" + \"when (`report_month`/`fiscal_*`)\" + \"what metric (`net_amount`/`revenue`/`expenses`/`line_share_*`)\"]\n  - 1.2.A.2 Handling of missing/abnormal/negative values [1 Point | Non-negative tests (`line_amount_nonneg`, `tax_amount_nonneg`, `invoice_*_nonneg`) and `not_null` exist, missing fields default to 0 or NULL]\n  - 1.2.A.3 Consistent definitions across models [1 Point | Revenue/expenses/net amount are consistent between P&L and GL; explicitly no recalculation or duplicate labeling]\n\n## Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n\n### Criterion 2.1 (Max 2 Points): Format/Structure Check\n- Path 2.1.A (2 Points)  \n  - 2.1.A.1 Upstream dependencies [1 Point | Each model declares `source_models`, cross-layer dependencies are restricted]\n  - 2.1.A.2 Granularity and field definitions [1 Point | Each model declares a `grain`, `columns` include field name + type, `not_null`/non-negative tests]\n\n### Criterion 2.2 (Max 2 Points): Dependency Closure\n- Path 2.2.A (2 Points)  \n  - 2.2.A.1 No dangling references [1 Point | All `source_models` point to valid `staging`/`intermediate` models]\n  - 2.2.A.2 No circular/improper cross-layer dependencies [1 Point | DAG is closed, unidirectional flow: `staging`→`intermediate`→`marts`]\n\n## Requirement III: Design Quality (3 Points)\n\n### Criterion 3.1 (Max 1 Point): Clear Naming\n- 3.1.1 Semantic naming [1 Point | Model and column names are intuitive (e.g., `xero__profit_and_loss_report`, `line_share_net`, `effective_tax_rate`)]\n\n### Criterion 3.2 (Max 2 Points): Reasonable Layering\n- 3.2.1 Clear layer responsibilities [1 Point | `staging` is only for cleaning; `intermediate` for aggregation/enrichment; `marts` for analytical output]\n- 3.2.2 Centralized business logic [1 Point | Metric logic is concentrated in `int_`/`marts`; `staging` contains no business rules]\n\n### Scoring Notes\n- Each sub-criterion is scored as 1 or 0; no evidence → 0 points.  \n- ALLOW_MULTI_MODEL=YES."}
{"id": "dacomp-de-arch-028", "question": "Our recent long-tail videos have shown good net subscriber growth within the \"first 30 days post-publication\" window, but management is concerned about whether this is driven by concentrated contributions from a very small number of countries or if it represents overall healthy growth.\n\nPlease help me break down the daily net subscribers, watch time, and like rate for each video at a `video-country-day` granularity for the first 1–30 days after its publication. The goal is to identify videos that exhibit \"steady net subscriber growth and increasing watch time but rely on an excessively high watch time share from a single country.\"\n\nFinally, provide a risk rating for each video:\n*   High (single-country contribution ≥70% and its share has been increasing in the past two weeks)\n*   Medium (single-country contribution is 50–70% or its share shows high volatility)\n*   Low (country distribution is well-balanced)\n\nAlso, please note the rolling trend for the last two weeks.", "rubric": "# YouTube Video-Country-Day 30-Day Growth and Risk Assessment Evaluation Criteria (Strict Version)\n\n## [Total Score | 23 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (16 Points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n## Requirement I: Business Alignment & Semantic Accuracy (16 Points)\n\n### Standard 1.1 (Max 13 Points): Feature Point Coverage\n- Path 1.1.A (13 Points): Complete Coverage\n  - 1.1.A.1 Sub-standard [1 Point | Judging Criteria]: Granularity is \"Video-Country-Day\"; a model defining `video_id` + `country_code` + `date_day` must exist, and all fields must be `not_null`.\n  - 1.1.A.2 Sub-standard [1 Point | Judging Criteria]: A video identifier `video_id` exists and is a primary key or part of a composite primary key, `not_null`.\n  - 1.1.A.3 Sub-standard [1 Point | Judging Criteria]: A country identifier `country_code` (ISO 2-letter) exists, `not_null`.\n  - 1.1.A.4 Sub-standard [1 Point | Judging Criteria]: A date column `date_day` (`DATE` type) exists, `not_null`, and serves as the day-granularity alignment field.\n  - 1.1.A.5 Sub-standard [1 Point | Judging Criteria]: A video published date `video_published_date` exists, used to calculate the relative days with `date_day`, `not_null`.\n  - 1.1.A.6 Sub-standard [1 Point | Judging Criteria]: A net subscribers count `net_subscribers` exists, is non-negative, and is consistent with the definition in the upstream `youtube__video_report`.\n  - 1.1.A.7 Sub-standard [1 Point | Judging Criteria]: A watch time metric (`total_watch_time_minutes` or `watch_time_minutes`) exists, non-negative.\n  - 1.1.A.8 Sub-standard [1 Point | Judging Criteria]: A like rate `like_rate` exists, and its value must be `BETWEEN 0 AND 1`.\n  - 1.1.A.9 Sub-standard [1 Point | Judging Criteria]: A country view percentage (`total_views_percentage`) exists, defined as the percentage of views from that country for the video on that day, and must be `BETWEEN 0 AND 1`.\n  - 1.1.A.10 Sub-standard [1 Point | Judging Criteria]: A 1–30 day window after publication is defined (based on `video_published_date` and `date_day`), and the window logic is computable.\n  - 1.1.A.11 Sub-standard [1 Point | Judging Criteria]: A rolling 14-day trend element (e.g., `rolling_14d_*` metrics) exists, `not_null`.\n  - 1.1.A.12 Sub-standard [1 Point | Judging Criteria]: A \"steady growth\" identification element (e.g., growth flag/threshold field) exists, clearly defined as ≥0 or boolean.\n  - 1.1.A.13 Sub-standard [1 Point | Judging Criteria]: A `risk_rating` exists, with its value limited to the enum {High, Medium, Low}.\n\n- Path 1.1.B (3 Points): Basic Coverage (Score one of the two, not cumulative with 1.1.A)\n  - 1.1.B.1 Sub-standard [1 Point | Judging Criteria]: The entity dimension covers at least two of the fields `video_id` and `country_code`.\n  - 1.1.B.2 Sub-standard [1 Point | Judging Criteria]: The time alignment dimension includes at least `date_day` or `video_published_date`.\n  - 1.1.B.3 Sub-standard [1 Point | Judging Criteria]: At least one aggregate/ratio metric (e.g., `net_subscribers`, `watch_time_minutes`, `like_rate`, `views_percentage`) exists, and the field is non-negative or has a range constraint.\n\n### Standard 1.2 (Max 3 Points): Metric/Feature Definition\n- Path 1.2.A (3 Points): Clear Definition\n  - 1.2.A.1 Sub-standard [1 Point | Judging Criteria]: Output items must include two types of elements (entity + time or statistical rule), e.g., `video_id` + `date_day` + aggregate metric.\n  - 1.2.A.2 Sub-standard [1 Point | Judging Criteria]: Constraints exist for missing/abnormal/duplicate/negative/future time values (e.g., `tests: not_null`, ≥0, `BETWEEN 0 AND 1`, date ≤ current date).\n  - 1.2.A.3 Sub-standard [1 Point | Judging Criteria]: Definitions are consistent across models (e.g., `like_rate`, `net_subscribers` maintain the same definition and unit in the intermediate and marts layers).\n\n## Requirement II: Technical Feasibility & Structural Completeness (4 Points)\n\n### Standard 2.1 (Max 2 Points): Format/Structure Check\n- Path 2.1.A (2 Points): Model's Four Essential Elements\n  - 2.1.A.1 Sub-standard [1 Point | Judging Criteria]: Declare upstream dependencies (`source_models`), with correct layering (`staging`→`intermediate`→`marts`).\n  - 2.1.A.2 Sub-standard [1 Point | Judging Criteria]: Each field definition includes name, `data_type`, and declares `tests` (`not_null`/range).\n\n### Standard 2.2 (Max 2 Points): Dependency Closure\n- Path 2.2.A (2 Points): Dependency Graph Can Be Built\n  - 2.2.A.1 Sub-standard [1 Point | Judging Criteria]: No dangling references (all upstream models exist and are resolvable).\n  - 2.2.A.2 Sub-standard [1 Point | Judging Criteria]: Clear boundaries for external sources (raw/staging source declarations are complete, unidirectional dependency).\n\n\n\n## Requirement III: Design Quality (3 Points)\n\n### Standard 3.1 (Max 1 Point): Clear Naming\n- 3.1.1 Sub-standard [1 Point | Judging Criteria]: Model and column names are semantically clear (e.g., `youtube__video_report`, `like_rate`, `net_subscribers`), avoiding ambiguity or mixed usage.\n\n### Standard 3.2 (Max 2 Points): Reasonable Layering\n- 3.2.1 Sub-standard [1 Point | Judging Criteria]: Clear layer responsibilities (`staging` for standardization; `intermediate` for business shaping; `marts` for analytical aggregation).\n- 3.2.2 Sub-standard [1 Point | Judging Criteria]: Business definitions are concentrated in the `marts` layer, avoiding \"God tables\"; models have a single, cohesive responsibility.\n\n### Scoring Notes\n- Each sub-standard is scored as 1/0; no evidence found → 0 points.\n- `ALLOW_MULTI_MODEL=YES`."}
{"id": "dacomp-de-arch-029", "question": "Our resolution time for high-priority tickets has become very long recently. Please help me understand clearly: across different organizations and creation channels, how do the `number of reassignments` (how many times an assignee is changed for the same ticket) and the `ratio of public to private comments` respectively affect the final resolution time? I don't need an academic paper, just an actionable conclusion: for example, in which organizations or channels does exceeding two reassignments increase the median resolution time and by how much, and what is the threshold below which the public comment ratio significantly slows down resolution efficiency. These findings should directly inform our adjustments to the triage strategy and communication standards.", "rubric": "# Evaluation Criteria for the Impact of Zendesk Ticket Reassignments and Comment Share on Resolution Time\n\n## [Total Score | 26 Points] Three Fixed Requirements\n- Requirement I: Business Alignment and Semantic Accuracy (19 Points)\n- Requirement II: Technical Feasibility and Structural Completeness (4 Points)\n- Requirement III: Design Quality (3 Points)\n\n\n\n## Requirement I: Business Alignment and Semantic Accuracy (19 Points)\n\n### Criterion 1.1 (Max 16 Points): Feature Point Coverage\n- Path 1.1.A (16 Points): Complete Coverage\n  - 1.1.A.1 Sub-criterion [1 Point | Condition]: Provides ticket-level granularity (`ticket_id` is unique, `not_null`).\n  - 1.1.A.2 Sub-criterion [1 Point | Condition]: Provides ticket creation time `created_at` (`not_null`), used as the starting point for resolution time.\n  - 1.1.A.3 Sub-criterion [1 Point | Condition]: Provides ticket final state time `resolved_at` or an equivalent field (`not_null`), sourced from `int_zendesk__ticket_status_transitions`.\n  - 1.1.A.4 Sub-criterion [1 Point | Condition]: Provides resolution time in minutes `resolution_minutes` (≥0 test).\n  - 1.1.A.5 Sub-criterion [1 Point | Condition]: Provides total comment count `comment_count` (≥0 test).\n  - 1.1.A.6 Sub-criterion [1 Point | Condition]: Provides public comment count `public_comment_count` (≥0 test).\n  - 1.1.A.7 Sub-criterion [1 Point | Condition]: Provides private comment count `private_comment_count` (≥0 test).\n  - 1.1.A.8 Sub-criterion [1 Point | Condition]: Can derive public comment share `public_comment_share = public / total` (derivation allowed, must be between 0–1).\n  - 1.1.A.9 Sub-criterion [1 Point | Condition]: Provides assignee change intervals (continuous `assignee` intervals, including start and end time columns).\n  - 1.1.A.10 Sub-criterion [1 Point | Condition]: Can derive reassignment count `reassignment_count` (based on `assignee` change count, ≥0).\n  - 1.1.A.11 Sub-criterion [1 Point | Condition]: Provides creation channel `created_channel` (`not_null`).\n  - 1.1.A.12 Sub-criterion [1 Point | Condition]: Provides organization ID `organization_id` (`not_null`).\n  - 1.1.A.13 Sub-criterion [1 Point | Condition]: Provides organization name or attributes (`organization_name`, `organization_tags`).\n  - 1.1.A.14 Sub-criterion [1 Point | Condition]: Can slice by organization × channel in the same analysis (both fields can be used together at the same granularity).\n  - 1.1.A.15 Sub-criterion [1 Point | Condition]: Final state determination follows the earliest time of solved/closed, consistent with the definition in the transitions model.\n  - 1.1.A.16 Sub-criterion [1 Point | Condition]: Provides status/priority dimensions (`status`, `priority`), `not_null`.\n\n- Path 1.1.B (8 Points): Basic Coverage (Choose one scoring path, not cumulative with 1.1.A)\n  - 1.1.B.1 Sub-criterion [1 Point | Condition]: Covers at least one entity type: ticket + organization or channel.\n  - 1.1.B.2 Sub-criterion [1 Point | Condition]: Covers at least one time field: `created_at` or `resolved_at`/equivalent field.\n  - 1.1.B.3 Sub-criterion [1 Point | Condition]: Covers at least one of the following: resolution time or comment count.\n  - 1.1.B.4 Sub-criterion [1 Point | Condition]: Distinguishes at least one type of comment: public or private.\n  - 1.1.B.5 Sub-criterion [1 Point | Condition]: Shows evidence of reassignments (e.g., an `assignee` change table).\n  - 1.1.B.6 Sub-criterion [1 Point | Condition]: Allows grouping by organization or channel.\n  - 1.1.B.7 Sub-criterion [1 Point | Condition]: Data definitions are consistent (resolution time calculation does not conflict with final state time).\n  - 1.1.B.8 Sub-criterion [1 Point | Condition]: Metric fields have non-negative constraint tests.\n\n\n\n### Criterion 1.2 (Max 3 Points): Metric/Feature Definitions\n- Path 1.2.A (3 Points): Clear Definitions\n  - 1.2.A.1 Sub-criterion [1 Point | Condition]: Output items include at least two types of elements (object/subject + time range or statistical rule), e.g., `ticket_id` + `created_at`/`resolved_at` + `resolution_minutes`.\n  - 1.2.A.2 Sub-criterion [1 Point | Condition]: Missing/abnormal values have default handling or tests (e.g., `comment_count` ≥0, `resolution_minutes` ≥0, `not_null`).\n  - 1.2.A.3 Sub-criterion [1 Point | Condition]: Definitions are consistent across outputs (final state time is consistent with resolution time; public/private comments are compatible with total comments).\n\n\n\n## Requirement II: Technical Feasibility and Structural Completeness (4 Points)\n\n### Criterion 2.1 (Max 2 Points): Format/Structure Check\n- Path 2.1.A (2 Points): Minimum Four Elements of a Model\n  - 2.1.A.1 Sub-criterion [1 Point | Condition]: Has upstream dependencies `source_models`.\n  - 2.1.A.2 Sub-criterion [1 Point | Condition]: Has field definitions + types (`columns` includes `data_type`).\n\n### Criterion 2.2 (Max 2 Points): Dependency Closure\n- Path 2.2.A (2 Points): Can Build Dependency Graph\n  - 2.2.A.1 Sub-criterion [1 Point | Condition]: No circular/self-references (follows unidirectional `staging`→`intermediate`→`marts`).\n  - 2.2.A.2 Sub-criterion [1 Point | Condition]: External source boundaries are clear (`staging.*` source definitions are complete).\n\n\n\n## Requirement III: Design Quality (3 Points)\n\n### Criterion 3.1 (Max 1 Point): Clear Naming\n- 3.1.1 Sub-criterion [1 Point | Condition]: Naming convention is consistent, prefixes are clear (`int_zendesk__`/`zendesk__`), `snake_case`.\n\n### Criterion 3.2 (Max 2 Points): Reasonable Layering\n- 3.2.1 Sub-criterion [1 Point | Condition]: Layer responsibilities are clear (`staging` for raw, `intermediate` for business logic, `marts` for analytics exposure).\n- 3.2.2 Sub-criterion [1 Point | Condition]: Business definitions are centralized (e.g., comment stats, resolution time are centralized in `int_zendesk__ticket_metrics` and exposed directly in marts).\n\n\n\n### Scoring Notes\n- Each sub-criterion is scored as 1 or 0; no evidence → 0 points.\n- ALLOW_MULTI_MODEL=YES."}
{"id": "dacomp-de-arch-030", "question": "For this quarter, of the `accrued Monthly Recurring Revenue (MRR)` for each account, how much remains uncollected 7, 30, and 60 days after invoicing? Please break this down by product, charge type (`Recurring`/`Non-Recurring`), and discount type. Also, separately flag invoices issued within 30 days before or after a cancellation/downgrade (`amendment`). I want to see:\n\n1) Which accounts have net MRR that is both eroded by discounts and has the slowest payment collection;\n2) The typical time lag distribution from invoicing to the first/full payment collection;\n3) The correlation with payment methods and their failure rates (e.g., card, ACH), so that we can adjust our payment terms and collection strategies.", "rubric": "# Zuora MRR Uncollected vs. Payment Lag Analysis Evaluation Criteria\n\n## [Total Score | 26 Points] Three Fixed Requirements\n- Requirement I: Business Alignment & Semantic Accuracy (19 points)\n- Requirement II: Technical Feasibility & Structural Completeness (4 points)\n- Requirement III: Design Quality (3 points)\n\n## Requirement I: Business Alignment & Semantic Accuracy (19 points)\n\n### Criterion 1.1 (Max 16 points): Feature Point Coverage\n- Path 1.1.A (16 points): Complete Coverage\n  - 1.1.A.1 Subsidiary (Account level/parent-child relationship) [1 point | `parent_account_id` exists, not_null]\n  - 1.1.A.2 Fiscal Period Alignment (Daily/Weekly/Monthly/Yearly buckets or date spine) [1 point | `date_day`/`date_week`/`date_month`/`date_year` exists, not_null]\n  - 1.1.A.3 Cash Flow (Payment/Refund) [1 point | Fields for payment amount and refund amount exist, amount ≥0]\n  - 1.1.A.4 AR Aging (Invoice due and uncollected/overdue) [1 point | `due_date`, `invoice_amount_unpaid`/`total_amount_past_due` exist, amount ≥0]\n  - 1.1.A.5 Gross Profit [1 point | Gross profit-related fields must be present, amount ≥0]\n  - 1.1.A.6 Expenses [1 point | Fields for expense definitions must be present, amount ≥0]\n  - 1.1.A.7 Vendor Payments [1 point | Vendor payment-related fields must be present, amount ≥0]\n  - 1.1.A.8 Contribution Share [1 point | A `contribution_share`-like field must be present, in the 0–1 range]\n  - 1.1.A.9 Corrective Actions (Credit Balance Adjustment) [1 point | Fields for credit balance adjustment amount and count exist, amount ≥0]\n  - 1.1.A.10 MRR/Non-MRR Classification (Monthly Net/Gross and Discount) [1 point | Differentiates `charge_type` and outputs MRR/Non-MRR net, gross, and discount amounts]\n  - 1.1.A.11 Product Dimension Breakdown [1 point | `product_id`/`name`/`category` exists, not_null]\n  - 1.1.A.12 Charge Type (Recurring/Non-Recurring) Dimension [1 point | `charge_type`/`charge_model` exists, not_null]\n  - 1.1.A.13 Discount Type/Amount Breakdown [1 point | `discount_amount`/`discount_amount_home_currency` exists, amount ≥0]\n  - 1.1.A.14 Flagging invoices ±30 days before/after cancellation/downgrade (amendment) [1 point | `invoice_date` and `amendment_*` fields exist and can be used for window calculation]\n  - 1.1.A.15 Distribution of lag from invoicing to first/full payment [1 point | `invoice_date` and `first_payment_date`/`most_recent_payment_date` exist, difference ≥0]\n  - 1.1.A.16 Association of payment method and failure rate [1 point | `payment_method_type`, `credit_card_type`, `ach_account_type`, `last_failed_sale_transaction_date` exist]\n  - 1.1.A.17 Accrued MRR definition (Service period alignment and daily buckets) [1 point | `service_start_*` and MRR amount fields exist, amount ≥0]\n  - 1.1.A.18 Currency and exchange rate definitions are consistent [1 point | Transaction currency amount, home currency amount, and `exchange_rate_date` exist, consistent across models]\n\n- Path 1.1.B (3 points): Basic Coverage (Scored only if 1.1.A is not met)\n  - 1.1.B.1 Entity dimensions available [1 point | At least two primary keys from Account/Invoice/Item/Subscription/Payment/PaymentMethod are included]\n  - 1.1.B.2 Time dimensions available [1 point | At least `date_day`/`date_month` or `invoice_date` is included]\n  - 1.1.B.3 Aggregation rules available [1 point | At least one amount aggregation and one count aggregation are included]\n\n### Criterion 1.2 (Max 3 points): Metric/Feature Definition\n- Path 1.2.A (3 points): Clear Definition\n  - 1.2.A.1 Output items have complete elements [1 point | At least three elements: \"by whom (entity id) + time range (day/month) + aggregation rule\"]\n  - 1.2.A.2 Handling of missing/abnormal/duplicate/negative values [1 point | Field includes `not_null` or `≥0` tests, duplicates handled by `is_most_recent_record` flag]\n  - 1.2.A.3 Consistent definitions across outputs [1 point | Unified date spine; consistent currency/home currency; `tax_amount_home_currency` consistent with invoice base]\n\n## Requirement II: Technical Feasibility & Structural Completeness (4 points)\n\n### Criterion 2.1 (Max 2 points): Format/Structure Check\n- Path 2.1.A (2 points): Model's Minimum Four Elements\n  - 2.1.A.1 Model Identifier [1 point | Each model declares a `name`]\n  - 2.1.A.2 Field Definition [1 point | Each column declares a name + type, `columns` includes `data_type`]\n\n### Criterion 2.2 (Max 2 points): Dependency Closure\n- Path 2.2.A (2 points): Dependency graph can be built\n  - 2.2.A.1 No dangling references [1 point | All `source_models` point to defined models]\n  - 2.2.A.2 Clear boundaries for external sources [1 point | `staging` only references `raw`, `intermediate` only references `staging`, `marts` only references `intermediate`]\n\n## Requirement III: Design Quality (3 points)\n\n### Criterion 3.1 (Max 1 point): Clear Naming\n- 3.1.1 Semantic Naming [1 point | Model and column names are intuitive, e.g., `zuora__account_overview`, `invoice_amount_unpaid`, `charge_type`]\n\n### Criterion 3.2 (Max 2 points): Reasonable Layering\n- 3.2.1 Clear layer responsibilities [1 point | Unidirectional layering from `staging`→`intermediate`→`marts` with clear responsibilities]\n- 3.2.2 Avoid \"God tables\" [1 point | Separation of detail and summary (`line_item_*` vs `*_overview`), no cross-layer coupling]\n\n### Scoring Notes\n- Each sub-criterion is scored as 1/0; no evidence found → 0 points.\n- ALLOW_MULTI_MODEL=YES."}
